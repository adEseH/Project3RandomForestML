{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some libraries and the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "import sklearn.model_selection as skm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.experimental import enable_halving_search_cv # because it is experimental we need this\n",
    "from sklearn.model_selection import GridSearchCV, HalvingGridSearchCV\n",
    "from ISLP import load_data, confusion_table\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, log_loss, confusion_matrix, classification_report)\n",
    "from sklearn.ensemble import (RandomForestRegressor, RandomForestClassifier)\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image, Markdown\n",
    "import graphviz\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statistics\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no idea what this is\n",
    "#data_index_example = pd.read_csv('../Project_datasets/data_index - example.csv')\n",
    "LPJ_GUESS_output = pd.read_csv('../Project_datasets/LPJ-GUESS_output_BERN1.csv')\n",
    "# main dataset\n",
    "data_index_2 = pd.read_csv('../Project_datasets/data_index_2.csv', quotechar=\"'\")\n",
    "data_index_2 = data_index_2.drop(columns=['Unnamed: 0'])\n",
    "data_index_2.set_index(['Lon','Lat'])\n",
    "LPJ_GUESS_output = LPJ_GUESS_output[~(data_index_2.isna().any(axis=1))] # delete rows with NaN\n",
    "data_index_2 = data_index_2[~(data_index_2.isna().any(axis=1))] # delete rows with NaN\n",
    "# precipitation by day and statistics\n",
    "Predaymean = pd.read_csv('../Project_datasets/Predaymean1961_1990.csv')\n",
    "Predaymean_statistics = pd.read_csv('../Project_datasets/Predaymean1961_1990_statics.csv')\n",
    "# maximum temperature by day and statistics\n",
    "Tmaxdaymean = pd.read_csv('../Project_datasets/Tmaxdaymean1961_1990.csv')\n",
    "Tmaxdaymean_statistics = pd.read_csv('../Project_datasets/Tmaxdaymean1961_1990_statics.csv')\n",
    "# minimum temperature by day and statistics\n",
    "Tmindaymean = pd.read_csv('../Project_datasets/Tmindaymean1961_1990.csv')\n",
    "Tmindaymean_statistics = pd.read_csv('../Project_datasets/Tmindaymean1961_1990_statics.csv')\n",
    "# mean temperature by day and statistics\n",
    "Tmpdaymean = pd.read_csv('../Project_datasets/Tmpdaymean1961_1990.csv')\n",
    "Tmpdaymean_statistics = pd.read_csv('../Project_datasets/Tmpdaymean1961_1990_statics.csv')\n",
    "# shortwave radiation flux\n",
    "Tswrfdaymean = pd.read_csv('../Project_datasets/Tswrfdaymean1961_1990.csv')\n",
    "Tswrfdaymean_statistics = pd.read_csv('../Project_datasets/Tswrfdaymean1961_1990_statics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridlist_pan_gfed_ISO3_UN = open('../Project_datasets/gridlist_pan_gfed_ISO3_UN.txt','r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the first entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short legend about the colums: \n",
    "\n",
    "Lon: Longitude<br>\n",
    "Lat: Latidude\n",
    "\n",
    "stuff about the souil texture: <br>\n",
    "\n",
    "clay: Clay in %<br>\n",
    "silt: Silt in % (sedimentary deposit that is formed when a river deposits the material it is carrying)<br>\n",
    "slay: slay in %(soil in eg. rainforest) <br>\n",
    "sand: sand in % %<br>\n",
    "orC: organic carbondioxid in %<br>\n",
    "CN: Cyanide in %<br>\n",
    "pH: pH<br>\n",
    "cell fraction: portion of cells / organic material in soil in % <br>\n",
    "\n",
    "\n",
    "Allways with season: Same as the tables below.\n",
    "\n",
    "tmax: maximum temperatur in K <br>\n",
    "tmin: min temperature in K <br>\n",
    "tmp:mean temperature in K <br>\n",
    "Pre: Precipitation, mm day-1 <br>\n",
    "tswrf: Total shortwave radiation flux, W m-2\n",
    "\n",
    "\n",
    "This is the GUESS output: \n",
    "\n",
    "NPP: net primary productivity (kg C m-2 year-1)<br>\n",
    "SoilR: soil respiration (kg C m-2 year-1)<br>\n",
    "MaxBiomeCmass: The maximum biomass from a single biome (kg C m-2)<br>\n",
    "MxbiomeLAI: The maximum leaf area index from a single biome (unitless)<br>\n",
    "VegC: Vegetation carbon poo (kg C m-2)l<br>\n",
    "LitterC: Litter carbon pool (kg C m-2)<br>\n",
    "SoilC: Soil carbon pool (kg C m-2)<br>\n",
    "Biome_Cmass: The biome type based on the maximum biomass (category)<br>\n",
    "Biome_LAI: The biome type based on the maximum LAI (category)<br>\n",
    "Biome_obs: The observed biome type (category)<br>\n",
    "\n",
    "Country codes\n",
    "\n",
    "GFED-region: Global Fire Emissions Database (https://www.un-spider.org/global-fire-emissions-database-gfed) <br>\n",
    "Pan_2007: Big reogion (Europa, Africa, Australia, USA, Russia, China, .... )<br>\n",
    "ISO3: Abbreviation for country <br>\n",
    "UN: Country code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {data_index_2.shape}')\n",
    "data_index_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Project_datasets/Readme.txt\",'r') as script:\n",
    "    speech = script.read().splitlines()\n",
    "\n",
    "count = 1\n",
    "for line in speech:\n",
    "    if False:\n",
    "        count+=1\n",
    "        if count % 2 == 0: #this is the remainder operator\n",
    "            print(line)\n",
    "    else:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LPJ_Guess_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {LPJ_GUESS_output.shape}')\n",
    "LPJ_GUESS_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pred (Precipitation, mm day-1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Predaymean.shape}')\n",
    "Predaymean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Predaymean_statistics.shape}')\n",
    "Predaymean_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tmp (Daily mean temperature, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmpdaymean.shape}')\n",
    "Tmpdaymean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmpdaymean_statistics.shape}')\n",
    "Tmpdaymean_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmaxdaymean.shape}')\n",
    "Tmaxdaymean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmaxdaymean_statistics.shape}')\n",
    "Tmaxdaymean_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmindaymean.shape}')\n",
    "Tmindaymean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmindaymean_statistics.shape}')\n",
    "Tmindaymean_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tswrf (Total shortwave radiation flux, W m-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tswrfdaymean.shape}')\n",
    "Tswrfdaymean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'shape: {Tswrfdaymean_statistics.shape}')\n",
    "Tswrfdaymean_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### County List and binome legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from file into a list-of-lists table.\n",
    "with open('../Project_datasets/gridlist_pan_gfed_ISO3_UN.txt') as file:\n",
    "    datatable = [line.split() for line in file.read().splitlines()]\n",
    "\n",
    "country_codes = pd.DataFrame(datatable, columns = ['Lon', 'Lat', 'GFED-region', 'Pan_2007', 'ISO3', 'UN'] )  \n",
    "country_codes = country_codes.drop(index = 0)\n",
    "country_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "with open(\"../Project_datasets/legend of biomes.txt\",'r') as script:\n",
    "    speech = script.read().splitlines()\n",
    "\n",
    "count = 1\n",
    "for line in speech:\n",
    "    if True:\n",
    "        count+=1\n",
    "        if count % 2 == 0: #this is the remainder operator\n",
    "            print(line)\n",
    "    else:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_index_2.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countplot to count the number of data for each biom: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plott - Biome_LAI: The biome type based on the maximum LAI (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplott - Biome_Cmass: The biome type based on the maximum biomass (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data_index_2, x=\"Lon\", y=\"Lat\", hue=\"Biome_obs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Some preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of samples of different biomes in different coutries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countrys_sample_size = data_index_2.groupby(['ISO3', 'Biome_obs']).size()   ##### NOT TESTED\n",
    "#countrys_sample_size = pd.DataFrame(countrys_sample_size)\n",
    "#countrys_sample_size.columns = [ \"Size\"]           # Maybe solve later\n",
    "countrys_sample_size.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samply size for countries\n",
    "countrys_sample_size.at[\"EGY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and table generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names my have to be improved\n",
    "def df_to_latex(df, filename = 'mytable'):  \n",
    "    s = df.style.to_latex(\n",
    "    column_format=\"rrrrr\", position=\"h\", position_float=\"centering\",\n",
    "    hrules=True, label=\"table:5\",\n",
    "    multirow_align=\"t\", multicol_align=\"r\"\n",
    "    )  \n",
    "        \n",
    "    with open('../Table/' + filename + '.tex', 'w') as f:\n",
    "        f.write(s)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "df = pd.DataFrame(dict(name=['Raphael', 'Donatello'],\n",
    "                       age=[26, 45],\n",
    "                       height=[181.23, 177.65]))\n",
    "\n",
    "df_to_latex(df, filename = 'ATable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(figure, figurename = 'my_plot'):\n",
    "    figure.savefig('../plots/' + figurename + '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "#save_plot(fig, figurename ='worldmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot function for statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_statistics(data, name_data = 'data'):\n",
    "    \n",
    "    obs_data = \"Biome_obs\"\n",
    "    fig, ax = plt.subplots(figsize = (8,4))\n",
    "    ax = sns.countplot(data, x = obs_data)\n",
    "    save_plot(fig, figurename = 'countplot_' + name_data + '_' + obs_data)\n",
    "    \n",
    "    \n",
    "    ## sample biomes\n",
    "    obs_data = \"Biome_obs_ISO3\"\n",
    "    fig, ax = plt.subplots(figsize = (8,4))\n",
    "    ax = sns.countplot(data, x = 'ISO3', hue = 'Biome_obs')\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    save_plot(fig, figurename = 'countplot_' + name_data + '_' + obs_data)\n",
    "    \n",
    "    ## scatterplot biome_Cmax and biome_LAI\n",
    "    obs_data = \"Biome_obs_Biome_Cmax\"\n",
    "    fig, ax = plt.subplots(figsize = (4,4))\n",
    "    ax = sns.scatterplot(data, x = \"Biome_Cmax\", y = \"Biome_obs\")\n",
    "    ax.set_yticks(range(1, 20,2))\n",
    "    save_plot(fig, figurename = 'scatterplot_' + name_data + '_' + obs_data)\n",
    "    \n",
    "    obs_data = \"Biome_obs_Biome_LAI\"\n",
    "    fig, ax = plt.subplots(figsize = (4,4))\n",
    "    ax = sns.scatterplot(data, x = \"Biome_LAI\", y = \"Biome_obs\")\n",
    "    ax.set_yticks(range(1, 20,2))\n",
    "    save_plot(fig, figurename = 'scatterplot_' + name_data + '_' + obs_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_index_2.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose countries. Random.org gave us the biomes\n",
    "- 17: desert\n",
    "- 16: Arid shrub/steppe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biome_list_3 = [17,16] # our chosen biomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plott - Biome_LAI: The biome type based on the maximum LAI (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_statistics(data_index_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data_index_2, x=\"Lon\", y=\"Lat\", hue=\"Biome_obs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that `NaN` appeard somewhere in `data_index_2`. The following is to find out where it occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(data_index_2.loc[:,data_index_2.isna().any()]))\n",
    "data_index_2[data_index_2.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose countries. Random.org gave us the biomes\n",
    "- 17: desert\n",
    "- 16: Arid shrub/steppe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biome_list_3 = [17,16] # our chosen biomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countplot: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_data = data_index_2.loc[data_index_2['Biome_obs'].isin(biome_list_3)]\n",
    "relevant_data = relevant_data[~(relevant_data.isna().any(axis=1))] # delete rows with NaN\n",
    "\n",
    "plot_statistics(relevant_data, name_data = 'relevant_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we choose Egypt to train and China as test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_train_3 = data_index_2['ISO3'] == 'EGY' # egypt is the chosen one\n",
    "region_test_3 = data_index_2['ISO3'] == 'CHN' # china is the chosen one, alternatively LBY (libya)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the action starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function generates X_test, Y_test and X_train, Y_train for the binary classification\n",
    "def model_create_XY(region_train, region_test, \\\n",
    "                     drop_columns=[], use_columns=None, \\\n",
    "                        objective='Biome_obs', objective_list=None, continuous_Y=False,\n",
    "                        objective_test=None):\n",
    "      \n",
    "      drop_columns = drop_columns.copy() + [objective] # we would not want our objective to appear in the training data\n",
    "      if not objective_test: objective_test = objective\n",
    "\n",
    "      # restrict to two biomes, restrict to country codes, remove any rows with NaN\n",
    "      if not continuous_Y:\n",
    "            if not objective_list: objective_list=list(data_index_2.loc[region_train][objective].drop_duplicates())\n",
    "            relevant_data_train = data_index_2.loc[data_index_2[objective].isin(objective_list)] # restrict to given biomes\n",
    "            # in case we want to use LPJ_guess output, write LPJ_ before the objective_list\n",
    "            if re.search('LPJ_',objective_test):\n",
    "                  objective_test = objective_test[4:]\n",
    "                  relevant_data_test = data_index_2.loc[LPJ_GUESS_output[objective_test].isin(objective_list)] # restrict to given biomes\n",
    "                  relevant_data_test[objective_test] = LPJ_GUESS_output.loc[LPJ_GUESS_output[objective_test].isin(objective_list)][objective_test] # restrict to given biomes\n",
    "            else:\n",
    "                  relevant_data_test = data_index_2.loc[data_index_2[objective_test].isin(objective_list)] # restrict to given biomes\n",
    "      else:\n",
    "            relevant_data_train = data_index_2\n",
    "            relevant_data_test = data_index_2\n",
    "            if re.search('LPJ_',objective_test):\n",
    "                  objective_test = objective_test[4:]\n",
    "                  relevant_data_test[objective_test] = LPJ_GUESS_output[objective_test]\n",
    "\n",
    "                  \n",
    "      relevant_data_train = relevant_data_train.loc[region_train] # restrict to training country\n",
    "      relevant_data_test = relevant_data_test.loc[region_test] # restrict to test country\n",
    "\n",
    "      drop_columns += ['MaxBiomeLAI','Biome_obs','Biome_LAI','Biome_Cmax',\n",
    "                       'Lon','Lat','Pan_2007','ISO3','UN','MaxBiomeCmax'] # drop these columns\n",
    "      if use_columns:\n",
    "            X_train = relevant_data_train[use_columns]\n",
    "            X_test = relevant_data_test[use_columns]\n",
    "      else:\n",
    "            X_train = relevant_data_train.drop(columns=drop_columns)\n",
    "            X_test = relevant_data_test.drop(columns=drop_columns)\n",
    "\n",
    "      feature_names = list(X_train.columns)\n",
    "\n",
    "      Y_train = relevant_data_train[objective]\n",
    "      Y_test = relevant_data_test[objective_test]\n",
    "\n",
    "      print(f\"length of training data: {Y_train.shape[0]}\")\n",
    "      print(f\"length of testing data: {Y_test.shape[0]}\")\n",
    "\n",
    "      return X_train, X_test, Y_train, Y_test, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for plotting (Permutation importance vs. purity importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_permutation_importance(clf, X, y, ax):\n",
    "    result = permutation_importance(clf, X, y, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    perm_sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "    ax.boxplot(\n",
    "        result.importances[perm_sorted_idx].T,\n",
    "        vert=False,\n",
    "        labels=X.columns[perm_sorted_idx],\n",
    "    )\n",
    "    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "    return ax\n",
    "\n",
    "def plot_permutation_importance_big_graph(clf, X_train, Y_train, feature_imp):\n",
    "\n",
    "    mdi_importances = pd.Series(clf.feature_importances_, index=X_train.columns)\n",
    "    # tree_importance_sorted_idx = np.argsort(clf.feature_importances_)\n",
    "    # tree_indices = np.arange(0, len(clf.feature_importances_)) + 0.5\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 12))\n",
    "    mdi_importances.sort_values().plot.barh(ax=ax1)\n",
    "    ax1.set_xlabel(\"Gini importance\")\n",
    "    plot_permutation_importance(clf, X_train, Y_train, ax2)\n",
    "    ax2.set_xlabel(\"Decrease in accuracy score\")\n",
    "    fig.suptitle(\n",
    "        \"Impurity-based vs. permutation importances on multicollinear features (train set)\"\n",
    "    )\n",
    "    _ = fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following generates, trains and evaluates the model\n",
    "def model_train_evaluate(X_train, X_test, Y_train, Y_test, feature_names, \\\n",
    "                         hyperparameter_tuning=False):\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=0)\n",
    "    \n",
    "    if hyperparameter_tuning==4: # for testing parameters manually\n",
    "        clf = RandomForestClassifier(random_state=0, max_depth=10, n_estimators=100, min_samples_leaf=1)\n",
    "    elif hyperparameter_tuning:\n",
    "        X_hyper, Y_hyper = X_train, Y_train\n",
    "        # X_hyper,_,Y_hyper,_ = skm.train_test_split(X_train, Y_train, train_size=300,random_state=0)\n",
    "        # TODO: improve on this\n",
    "        kfold3 = skm.KFold(3,\n",
    "                        random_state=1,\n",
    "                        shuffle=True) # let's try fewer fold to save running time\n",
    "\n",
    "        fineness = 4 # determines how many parameters should be tested\n",
    "        params = {\n",
    "                'max_depth': np.linspace(5, 15, fineness).astype('int'), # (5,15)\n",
    "                'n_estimators': np.linspace(50, 150, fineness).astype('int'), # (10,40) CV number of trees, keep this low for part 4\n",
    "                'min_samples_leaf': np.linspace(1, 5, fineness).astype('int'), # minimum leaf number\n",
    "                }\n",
    "        if False:\n",
    "            rfc_gscv = GridSearchCV(clf, param_grid = params, scoring = \"accuracy\",\n",
    "                                        cv = kfold3 )\n",
    "        else:\n",
    "            # Alternatively use HalvingGridSearchCV which is equally abismal in its performance\n",
    "            rfc_gscv = HalvingGridSearchCV(clf, param_grid = params, scoring = \"accuracy\",\n",
    "                                        cv = kfold3, min_resources=20, max_resources=30)\n",
    "        \n",
    "        # Fit the model\n",
    "        model_rfc = rfc_gscv.fit(X_hyper, Y_hyper)\n",
    "\n",
    "        # Model best estimator\n",
    "        max_depths=model_rfc.best_estimator_.get_params()[\"max_depth\"]\n",
    "        max_trees= model_rfc.best_estimator_.get_params()[\"n_estimators\"]\n",
    "        min_samples_leaf= model_rfc.best_estimator_.get_params()[\"min_samples_leaf\"]\n",
    "        max_cvs= rfc_gscv.best_score_\n",
    "        print(\"Max Depth: \", max_depths)\n",
    "        print(\"Max Trees: \",max_trees)\n",
    "        print(\"Min Leafs: \",min_samples_leaf)\n",
    "        print(\"Max CV: \",max_cvs)\n",
    "\n",
    "        clf = RandomForestClassifier(random_state=0, max_depth=max_depths, n_estimators=max_trees, min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "    # Some performance \n",
    "    score_rfc_train = accuracy_score(Y_train, clf.predict(X_train))\n",
    "    print('Accuracy of RandomForestClassifier on train data: {:.4f}'.format(score_rfc_train))\n",
    "    kfold = skm.KFold(3, random_state=1, shuffle=True)\n",
    "    # This becomes computationally quite expensive for large training sets\n",
    "    scores_rfc_val = cross_val_score(clf, X_train, Y_train, cv=kfold)\n",
    "    print(\"Accuracy RandomForestClassifier on cross validation: %0.4f ( %0.2f)\" % (scores_rfc_val.mean(), scores_rfc_val.std()))\n",
    "    score_rfc_test = accuracy_score(Y_test, clf.predict(X_test))\n",
    "    print('Accuracy of RandomForestClassifier on test data: {:.4f}'.format(score_rfc_test))\n",
    "    \n",
    "    # confusion table\n",
    "    display(Markdown('---\\n The confusion table'))\n",
    "    display(confusion_table(clf.predict(X_test),\n",
    "                            Y_test))\n",
    "    \n",
    "    classreport = classification_report(Y_test, clf.predict(X_test), zero_division = np.nan, output_dict=True)\n",
    "    classreport = pd.DataFrame(classreport).transpose()\n",
    "    display(classreport)\n",
    "    \n",
    "    # display importance table\n",
    "    display(Markdown('---\\n The feature importance'))\n",
    "    feature_impo = clf.feature_importances_\n",
    "    feature_imp = pd.DataFrame(\n",
    "    {'importance':clf.feature_importances_},\n",
    "    index=feature_names)\n",
    "    display(feature_imp.sort_values(by='importance', ascending=False))\n",
    "    \n",
    "    \n",
    "    # Plot Impurity-based vs. Permutation importance\n",
    "    #plot_permutation_importance_big_graph(clf, X_train, Y_train, feature_imp)\n",
    "    \n",
    "    display(Markdown('---'))\n",
    "\n",
    "def regression_train_evaluate(X_train, X_test, Y_train, Y_test, feature_names, \\\n",
    "                         hyperparameter_tuning=False):\n",
    "    reg = RandomForestRegressor()\n",
    "    reg.fit(X_train, Y_train)\n",
    "\n",
    "    score_reg_train = reg.score(X_train, Y_train)\n",
    "    print('Accuracy of RandomForestRegressor on train data: {:.4f}'.format(score_reg_train))\n",
    "    # The following takes way to long for large training sets\n",
    "    # kfold = skm.KFold(3, random_state=1, shuffle=True)\n",
    "    # scores_reg_val = cross_val_score(reg, X_train, Y_train, cv=kfold)\n",
    "    # print(\"Accuracy RandomForestRegressor on cross validation: %0.4f ( %0.2f)\" % (scores_reg_val.mean(), scores_reg_val.std()))\n",
    "    score_reg_test = reg.score(X_test, Y_test)\n",
    "    print('Accuracy of RandomForestRegressor on test data: {:.4f}'.format(score_reg_test))\n",
    "\n",
    "    # MSE\n",
    "    Y_hat = reg.predict(X_test)\n",
    "    mse = np.mean((Y_test - Y_hat)**2)\n",
    "    print(f'MSE for training data {mse}')\n",
    "\n",
    "    fig, ax = subplots()\n",
    "    ax.scatter(Y_hat, Y_test)\n",
    "    ax.set_xlabel('predicted sample $\\hat{Y}$')\n",
    "    ax.set_ylabel('exact sample $Y_{test}$')\n",
    "    maxval = max([np.max(Y_test),np.max(Y_hat)])\n",
    "    ax.plot([0,maxval],[0,maxval],color='red')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(Y_test-Y_hat, bins=50, density=True)\n",
    "    ax.set_xlabel('residue $Y_{test}-\\hat{Y}$')\n",
    "    ax.set_ylabel('Share of samples')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the training data, test data and the model\n",
    "def model_run(region_train, region_test, objective='Biome_obs', objective_list=None, \\\n",
    "                drop_columns=[], use_columns=None, \\\n",
    "                hyperparameter_tuning=False, continuous_Y=False, objective_test=None):\n",
    "    X_train, X_test, Y_train, Y_test, feature_names = model_create_XY(region_train, \\\n",
    "                        region_test, drop_columns, use_columns, objective, \\\n",
    "                              objective_list, continuous_Y, objective_test)\n",
    "    if continuous_Y:\n",
    "        regression_train_evaluate(X_train, X_test, Y_train, Y_test, feature_names, \\\n",
    "                         hyperparameter_tuning)\n",
    "    else:\n",
    "        model_train_evaluate(X_train, X_test, Y_train, Y_test, feature_names, \\\n",
    "                         hyperparameter_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_3, region_test_3, objective_list=biome_list_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some evaluation of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now run the model whilst removing the medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_median = [feature_name for feature_name in list(data_index_2) if feature_name[-6:]=='Median']\n",
    "print(f'We dropped the features: {features_median}')\n",
    "\n",
    "model_run(region_train_3, region_test_3, objective_list=biome_list_3, drop_columns=features_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently dropping the medians makes the prediction more accurate. Let's see how important the weather is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_weather = [feature_name for feature_name in list(data_index_2) if \\\n",
    "                   re.search('Fall|Summer|Winter|Spring',feature_name)]\n",
    "print(f'We dropped the features: {features_weather}')\n",
    "display(Markdown('---'))\n",
    "\n",
    "model_run(region_train_3, region_test_3, objective_list=biome_list_3, drop_columns=features_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not important at all. Dropping it makes our model more accurate. But this has to do with the fact that we chose two countries at different latitudes. What effect do the different seasons have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = ['Fall','Summer','Winter','Spring']\n",
    "features_seasons = [[feature_name for feature_name in list(data_index_2) if \\\n",
    "                   re.search(season,feature_name)] for season in seasons]\n",
    "\n",
    "for i,season in enumerate(seasons):\n",
    "    print(f'Dropping season {season}')\n",
    "    model_run(region_train_3, region_test_3, objective_list=biome_list_3, drop_columns=features_seasons[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping autumn or summer had the biggest impact. Dropping spring and winter the least. Dropping the seasons does negatively effect our model. Now we test the different weather parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_params = ['pre','tmp|tmin|tmax','tswrf']\n",
    "features_weather_params = [[feature_name for feature_name in list(data_index_2) if \\\n",
    "                   re.search(weather_param,feature_name)] for weather_param in weather_params]\n",
    "\n",
    "for i,weather_param in enumerate(weather_params):\n",
    "    print(f'Dropping the weather parameters {weather_param}')\n",
    "    model_run(region_train_3, region_test_3, objective_list=biome_list_3, drop_columns=features_weather_params[i].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the weather parameters has a striking impact on performance. Now lets see if the model works only with weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We only use the features: {features_weather}')\n",
    "display(Markdown('---'))\n",
    "\n",
    "model_run(region_train_3, region_test_3, objective_list=biome_list_3, use_columns=features_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs terribly as would be expected if one removed the most important parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the significant negative impact on the learning outcome by choosing regions at very different latitudes we chose as regions\n",
    "- for training: Canada\n",
    "- for testing: Russia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_train_4 = data_index_2['Pan_2007'] == 'Canada'\n",
    "region_test_4 = data_index_2['Pan_2007'] == 'Russia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is relatively bad performance. Fiddling with the hyperparameters changes a lot though the\n",
    "hyperparameter optimisation is far from optimal. Let's see how good it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, hyperparameter_tuning=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not too good in this case. But when we were previously comparing Africa with China fiddling with the parameters actually did improve things. In the following we will thus disable the hyperparameter tuning. Let's see if things improve if we drop the features with medians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, drop_columns=features_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did not make much of a difference. Let's see what happens if we remove the entire weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, drop_columns=features_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently the weather is important, but not very. Let's see what effect the different seasons have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,season in enumerate(seasons):\n",
    "    print(f'Dropping season {season}')\n",
    "    model_run(region_train_4, region_test_4, drop_columns=features_seasons[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that dropping fall decreased the performance the most whilst dropping the summer actually increased performance slightly. Let's see what happens if we use only weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_weather)\n",
    "model_run(region_train_4, region_test_4, use_columns=features_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn's out that using only weather to classify biomes does not really go well. Let's see if the model works better at classifying 'Biome_Cmax'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, objective='Biome_Cmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is quite decent performance though not as good as with biome_obs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.6\n",
    "\n",
    "Test the model trained with 'Biome_Cmax' on 'Biome_obs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, objective='Biome_Cmax', objective_test='Biome_obs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who would have thought? The model performs as badly as an antigen test measuring temperature. Now test the model trained on 'Biome_obs' on 'Biome_Cmax' (and expect a similar result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, objective='Biome_obs', objective_test='Biome_Cmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the result is just as bad. What a surprise. Now compare our model with LPJ-Guess output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for objective in ['Biome_obs','Biome_Cmax']:\n",
    "    print(f'Comparing LPJ-Guess with our model for the parameter {objective}')\n",
    "    model_run(region_train_4, region_test_4, objective=objective, objective_test=f'LPJ_{objective}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who would have thought that our model works badly in this case (duh)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we creatively use the same test and validation set as in part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_train_5 = region_train_4\n",
    "region_test_5 = region_test_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_5, region_test_5, objective='NPP', continuous_Y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pretty decent outcome. Let's see how it behaves with 'VegC'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_5, region_test_5, objective='VegC', continuous_Y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'VegC' seems to perform worse. We analyse the importance of the weather on the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_5, region_test_5, objective='NPP', continuous_Y=True, drop_columns=features_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5.3\n",
    "We compare the model with the LPJ_Guess output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for objective in ['NPP','VegC']:\n",
    "    print(f'Comparing LPJ-Guess with our model for the parameter {objective}')\n",
    "    model_run(region_train_5, region_test_5, objective=objective, continuous_Y=True, objective_test=f'LPJ_{objective}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
