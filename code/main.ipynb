{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some libraries and the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "import sklearn.model_selection as skm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.experimental import enable_halving_search_cv # because it is experimental we need this\n",
    "from sklearn.model_selection import GridSearchCV, HalvingGridSearchCV\n",
    "from ISLP import load_data, confusion_table\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, log_loss, confusion_matrix)\n",
    "from sklearn.ensemble import (RandomForestRegressor, RandomForestClassifier)\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image, Markdown\n",
    "import graphviz\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statistics\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main dataset\n",
    "data_index_2 = pd.read_csv('../Project_datasets/data_index_2.csv', quotechar=\"'\")\n",
    "data_index_2 = data_index_2.drop(columns=['Unnamed: 0'])\n",
    "data_index_2.set_index(['Lon','Lat'])\n",
    "# no idea what this is\n",
    "#data_index_example = pd.read_csv('../Project_datasets/data_index - example.csv')\n",
    "LPJ_GUESS_output = pd.read_csv('../Project_datasets/LPJ-GUESS_output_BERN1.csv')\n",
    "# precipitation by day and statistics\n",
    "Predaymean = pd.read_csv('../Project_datasets/Predaymean1961_1990.csv')\n",
    "Predaymean_statistics = pd.read_csv('../Project_datasets/Predaymean1961_1990_statics.csv')\n",
    "# maximum temperature by day and statistics\n",
    "Tmaxdaymean = pd.read_csv('../Project_datasets/Tmaxdaymean1961_1990.csv')\n",
    "Tmaxdaymean_statistics = pd.read_csv('../Project_datasets/Tmaxdaymean1961_1990_statics.csv')\n",
    "# minimum temperature by day and statistics\n",
    "Tmindaymean = pd.read_csv('../Project_datasets/Tmindaymean1961_1990.csv')\n",
    "Tmindaymean_statistics = pd.read_csv('../Project_datasets/Tmindaymean1961_1990_statics.csv')\n",
    "# mean temperature by day and statistics\n",
    "Tmpdaymean = pd.read_csv('../Project_datasets/Tmpdaymean1961_1990.csv')\n",
    "Tmpdaymean_statistics = pd.read_csv('../Project_datasets/Tmpdaymean1961_1990_statics.csv')\n",
    "# shortwave radiation flux\n",
    "Tswrfdaymean = pd.read_csv('../Project_datasets/Tswrfdaymean1961_1990.csv')\n",
    "Tswrfdaymean_statistics = pd.read_csv('../Project_datasets/Tswrfdaymean1961_1990_statics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridlist_pan_gfed_ISO3_UN = open('../Project_datasets/gridlist_pan_gfed_ISO3_UN.txt','r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the first entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short legend about the colums: \n",
    "\n",
    "Lon: Longitude<br>\n",
    "Lat: Latidude\n",
    "\n",
    "stuff about the souil texture: <br>\n",
    "\n",
    "clay: Clay in %<br>\n",
    "silt: Silt in % (sedimentary deposit that is formed when a river deposits the material it is carrying)<br>\n",
    "slay: slay in %(soil in eg. rainforest) <br>\n",
    "sand: sand in % %<br>\n",
    "orC: organic carbondioxid in %<br>\n",
    "CN: Cyanide in %<br>\n",
    "pH: pH<br>\n",
    "cell fraction: portion of cells / organic material in soil in % <br>\n",
    "\n",
    "\n",
    "Allways with season: Same as the tables below.\n",
    "\n",
    "tmax: maximum temperatur in K <br>\n",
    "tmin: min temperature in K <br>\n",
    "tmp:mean temperature in K <br>\n",
    "Pre: Precipitation, mm day-1 <br>\n",
    "tswrf: Total shortwave radiation flux, W m-2\n",
    "\n",
    "\n",
    "This is the GUESS output: \n",
    "\n",
    "NPP: net primary productivity (kg C m-2 year-1)<br>\n",
    "SoilR: soil respiration (kg C m-2 year-1)<br>\n",
    "MaxBiomeCmass: The maximum biomass from a single biome (kg C m-2)<br>\n",
    "MxbiomeLAI: The maximum leaf area index from a single biome (unitless)<br>\n",
    "VegC: Vegetation carbon poo (kg C m-2)l<br>\n",
    "LitterC: Litter carbon pool (kg C m-2)<br>\n",
    "SoilC: Soil carbon pool (kg C m-2)<br>\n",
    "Biome_Cmass: The biome type based on the maximum biomass (category)<br>\n",
    "Biome_LAI: The biome type based on the maximum LAI (category)<br>\n",
    "Biome_obs: The observed biome type (category)<br>\n",
    "\n",
    "Country codes\n",
    "\n",
    "GFED-region: Global Fire Emissions Database (https://www.un-spider.org/global-fire-emissions-database-gfed) <br>\n",
    "Pan_2007: Big reogion (Europa, Africa, Australia, USA, Russia, China, .... )<br>\n",
    "ISO3: Abbreviation for country <br>\n",
    "UN: Country code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {data_index_2.shape}')\n",
    "data_index_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Project_datasets/Readme.txt\",'r') as script:\n",
    "    speech = script.read().splitlines()\n",
    "\n",
    "count = 1\n",
    "for line in speech:\n",
    "    if False:\n",
    "        count+=1\n",
    "        if count % 2 == 0: #this is the remainder operator\n",
    "            print(line)\n",
    "    else:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LPJ_Guess_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {LPJ_GUESS_output.shape}')\n",
    "LPJ_GUESS_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pred (Precipitation, mm day-1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Predaymean.shape}')\n",
    "Predaymean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Predaymean_statistics.shape}')\n",
    "Predaymean_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tmp (Daily mean temperature, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmpdaymean.shape}')\n",
    "Tmpdaymean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmpdaymean_statistics.shape}')\n",
    "Tmpdaymean_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmaxdaymean.shape}')\n",
    "Tmaxdaymean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmaxdaymean_statistics.shape}')\n",
    "Tmaxdaymean_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmindaymean.shape}')\n",
    "Tmindaymean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmindaymean_statistics.shape}')\n",
    "Tmindaymean_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tswrf (Total shortwave radiation flux, W m-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tswrfdaymean.shape}')\n",
    "Tswrfdaymean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'shape: {Tswrfdaymean_statistics.shape}')\n",
    "Tswrfdaymean_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### County List and binome legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from file into a list-of-lists table.\n",
    "with open('../Project_datasets/gridlist_pan_gfed_ISO3_UN.txt') as file:\n",
    "    datatable = [line.split() for line in file.read().splitlines()]\n",
    "\n",
    "country_codes = pd.DataFrame(datatable, columns = ['Lon', 'Lat', 'GFED-region', 'Pan_2007', 'ISO3', 'UN'] )  \n",
    "country_codes = country_codes.drop(index = 0)\n",
    "country_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "with open(\"../Project_datasets/legend of biomes.txt\",'r') as script:\n",
    "    speech = script.read().splitlines()\n",
    "\n",
    "count = 1\n",
    "for line in speech:\n",
    "    if True:\n",
    "        count+=1\n",
    "        if count % 2 == 0: #this is the remainder operator\n",
    "            print(line)\n",
    "    else:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_index_2.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countplot to count the number of data for each biom: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lineplot \n",
    "g = sns.countplot(data_index_2, x = \"Biome_obs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plott - Biome_LAI: The biome type based on the maximum LAI (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data_index_2, x = \"MaxBiomeLAI\", y = \"Biome_obs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplott - Biome_Cmass: The biome type based on the maximum biomass (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data_index_2, x = \"MaxBiomeCmax\", y = \"Biome_obs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data_index_2, x=\"Lon\", y=\"Lat\", hue=\"Biome_obs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Some preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of samples of different biomes in different coutries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countrys_sample_size = data_index_2.groupby(['ISO3', 'Biome_obs']).size()   ##### NOT TESTED\n",
    "#countrys_sample_size = pd.DataFrame(countrys_sample_size)\n",
    "#countrys_sample_size.columns = [ \"Size\"]           # Maybe solve later\n",
    "countrys_sample_size.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samply size for countries\n",
    "countrys_sample_size.at['EGY']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that `NaN` appeard somewhere in `data_index_2`. The following is to find out where it occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(data_index_2.loc[:,data_index_2.isna().any()]))\n",
    "data_index_2[data_index_2.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose countries. Random.org gave us the biomes\n",
    "- 17: desert\n",
    "- 16: Arid shrub/steppe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biome_list_3 = [17,16] # our chosen biomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countplot: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_data = data_index_2.loc[data_index_2['Biome_obs'].isin(biome_list_3)]\n",
    "relevant_data = relevant_data[~(relevant_data.isna().any(axis=1))] # delete rows with NaN\n",
    "fig, ax = plt.subplots(figsize = (8,4))\n",
    "ax = sns.countplot( relevant_data, x = 'ISO3', hue = 'Biome_obs')\n",
    "ax.tick_params(axis='x', rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we choose Egypt to train and China as test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_train_3 = data_index_2['ISO3'] == 'EGY' # egypt is the chosen one\n",
    "region_test_3 = data_index_2['ISO3'] == 'CHN' # china is the chosen one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the action starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function generates X_test, Y_test and X_train, Y_train for the binary classification\n",
    "def model_create_XY(region_train, region_test, \\\n",
    "                     drop_columns=[], use_columns=None, \\\n",
    "                        objective='Biome_obs', objective_list=None, continuous_Y=False):\n",
    "      \n",
    "      drop_columns += [objective] # we would not want our objective to appear in the training data\n",
    "\n",
    "      # restrict to two biomes, restrict to country codes, remove any rows with NaN\n",
    "      relevant_data = data_index_2\n",
    "      if not continuous_Y:\n",
    "            if not objective_list: objective_list=list(data_index_2.loc[region_train][objective].drop_duplicates())\n",
    "            relevant_data = relevant_data.loc[data_index_2[objective].isin(objective_list)] # restrict to given biomes\n",
    "      relevant_data = relevant_data[~(relevant_data.isna().any(axis=1))] # delete rows with NaN\n",
    "      relevant_data_train = relevant_data.loc[region_train] # restrict to training country\n",
    "      relevant_data_test = relevant_data.loc[region_test] # restrict to test country\n",
    "\n",
    "      drop_columns += ['MaxBiomeLAI','Biome_obs','Biome_LAI','Biome_Cmax',\n",
    "                       'Lon','Lat','Pan_2007','ISO3','UN','MaxBiomeCmax'] # drop these columns\n",
    "      if use_columns:\n",
    "            X_train = relevant_data_train[use_columns]\n",
    "            X_test = relevant_data_test[use_columns]\n",
    "      else:\n",
    "            X_train = relevant_data_train.drop(columns=drop_columns)\n",
    "            X_test = relevant_data_test.drop(columns=drop_columns)\n",
    "\n",
    "      feature_names = list(X_train.columns)\n",
    "\n",
    "      Y_train = relevant_data_train[objective]\n",
    "      Y_test = relevant_data_test[objective]\n",
    "\n",
    "      print(f\"length of training data: {Y_train.shape[0]}\")\n",
    "      print(f\"length of testing data: {Y_test.shape[0]}\")\n",
    "\n",
    "      return X_train, X_test, Y_train, Y_test, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for plotting (Permutation importance vs. purity importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_permutation_importance(clf, X, y, ax):\n",
    "    result = permutation_importance(clf, X, y, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    perm_sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "    ax.boxplot(\n",
    "        result.importances[perm_sorted_idx].T,\n",
    "        vert=False,\n",
    "        labels=X.columns[perm_sorted_idx],\n",
    "    )\n",
    "    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following generates, trains and evaluates the model\n",
    "def model_train_evaluate(X_train, X_test, Y_train, Y_test, feature_names, \\\n",
    "                         hyperparameter_tuning=False):\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=0)\n",
    "    \n",
    "    if hyperparameter_tuning:\n",
    "        X_hyper, Y_hyper = X_train, Y_train\n",
    "        # X_hyper,_,Y_hyper,_ = skm.train_test_split(X_train, Y_train, train_size=300,random_state=0)\n",
    "        # TODO: improve on this\n",
    "        kfold3 = skm.KFold(3,\n",
    "                        random_state=1,\n",
    "                        shuffle=True) # let's try fewer fold to save running time\n",
    "\n",
    "        fineness = 4 # determines how many parameters should be tested\n",
    "        params = {\n",
    "                'max_depth': np.linspace(5, 15, fineness).astype('int'),\n",
    "                'n_estimators': np.linspace(10, 40, fineness).astype('int'), # CV number of trees, keep this low for part 4\n",
    "                'min_samples_leaf': np.linspace(1, 5, fineness).astype('int'), # minimum leaf number\n",
    "                }\n",
    "        if False:\n",
    "            rfc_gscv = GridSearchCV(clf, param_grid = params, scoring = \"accuracy\",\n",
    "                                        cv = kfold3 )\n",
    "        else:\n",
    "            # Alternatively use HalvingGridSearchCV which is equally abismal in its performance\n",
    "            rfc_gscv = HalvingGridSearchCV(clf, param_grid = params, scoring = \"accuracy\",\n",
    "                                        cv = kfold3, min_resources=20, max_resources=30)\n",
    "        \n",
    "        # Fit the model\n",
    "        model_rfc = rfc_gscv.fit(X_hyper, Y_hyper)\n",
    "\n",
    "        # Model best estimator\n",
    "        max_depths=model_rfc.best_estimator_.get_params()[\"max_depth\"]\n",
    "        max_trees= model_rfc.best_estimator_.get_params()[\"n_estimators\"]\n",
    "        min_samples_leaf= model_rfc.best_estimator_.get_params()[\"min_samples_leaf\"]\n",
    "        max_cvs= rfc_gscv.best_score_\n",
    "        print(\"Max Depth: \", max_depths)\n",
    "        print(\"Max Trees: \",max_trees)\n",
    "        print(\"Min Leafs: \",min_samples_leaf)\n",
    "        print(\"Max CV: \",max_cvs)\n",
    "\n",
    "        clf = RandomForestClassifier(random_state=0, max_depth=max_depths, n_estimators=max_trees, min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "    # Some performance \n",
    "    score_rfc_train = accuracy_score(Y_train, clf.predict(X_train))\n",
    "    print('Accuracy of RandomForestClassifier on train data: {:.4f}'.format(score_rfc_train))\n",
    "    kfold = skm.KFold(3, random_state=1, shuffle=True)\n",
    "    # This becomes computationally quite expensive for large training sets\n",
    "    scores_rfc_val = cross_val_score(clf, X_train, Y_train, cv=kfold)\n",
    "    print(\"Accuracy RandomForestClassifier on cross validation: %0.4f ( %0.2f)\" % (scores_rfc_val.mean(), scores_rfc_val.std()))\n",
    "    score_rfc_test = accuracy_score(Y_test, clf.predict(X_test))\n",
    "    print('Accuracy of RandomForestClassifier on test data: {:.4f}'.format(score_rfc_test))\n",
    "    \n",
    "    # confusion table\n",
    "    display(Markdown('---\\n The confusion table'))\n",
    "    display(confusion_table(clf.predict(X_test),\n",
    "                            Y_test))\n",
    "    \n",
    "    conftable = confusion_matrix(clf.predict(X_test), Y_test) # gives same results as confusion_table\n",
    "    tp_and_fn = conftable.sum(0)\n",
    "    tp_and_fp = conftable.sum(1)\n",
    "    tp = conftable.diagonal()\n",
    "    \n",
    "    # Recall and precision\n",
    "    display(Markdown('---\\n Recall: proportion classified as True/ False of all True/Falls || [0,0]/ ([0,0] + [1,0]) || [1,1]/([0,1] + [1,1]'))\n",
    "    recall = tp / tp_and_fn \n",
    "    print('along coulumns', '\\nF:' , recall[0] ,'\\nT:' , recall[1])\n",
    "    display(Markdown('---\\n Precission: proportion that werre True/ False of all samples predicted as True/ falls || [0,0]/ ([0,0] + [0,1]) || [1,1]/([1,0] + [1,1]'))\n",
    "    precision = tp / tp_and_fp\n",
    "    print('along rows','\\nF:' , precision[0] ,'\\nT:' , precision[1])\n",
    "    \n",
    "    # display importance table\n",
    "    display(Markdown('---\\n The feature importance'))\n",
    "    feature_imp = pd.DataFrame(\n",
    "    {'importance':clf.feature_importances_},\n",
    "    index=feature_names)\n",
    "    display(feature_imp.sort_values(by='importance', ascending=False))\n",
    "    \n",
    "    # Plot Impurity-based vs. Permutation importance\n",
    "    mdi_importances = pd.Series(clf.feature_importances_, index=X_train.columns)\n",
    "    # tree_importance_sorted_idx = np.argsort(clf.feature_importances_)\n",
    "    # tree_indices = np.arange(0, len(clf.feature_importances_)) + 0.5\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 12))\n",
    "    mdi_importances.sort_values().plot.barh(ax=ax1)\n",
    "    ax1.set_xlabel(\"Gini importance\")\n",
    "    plot_permutation_importance(clf, X_train, Y_train, ax2)\n",
    "    ax2.set_xlabel(\"Decrease in accuracy score\")\n",
    "    fig.suptitle(\n",
    "        \"Impurity-based vs. permutation importances on multicollinear features (train set)\"\n",
    "    )\n",
    "    _ = fig.tight_layout()\n",
    "    \n",
    "    display(Markdown('---'))\n",
    "\n",
    "def regression_train_evaluate(X_train, X_test, Y_train, Y_test, feature_names, \\\n",
    "                         hyperparameter_tuning=False):\n",
    "    reg = RandomForestRegressor()\n",
    "    reg.fit(X_train, Y_train)\n",
    "\n",
    "    score_reg_train = reg.score(X_train, Y_train)\n",
    "    print('Accuracy of RandomForestRegressor on train data: {:.4f}'.format(score_reg_train))\n",
    "    # The following takes way to long for large training sets\n",
    "    # kfold = skm.KFold(3, random_state=1, shuffle=True)\n",
    "    # scores_reg_val = cross_val_score(reg, X_train, Y_train, cv=kfold)\n",
    "    # print(\"Accuracy RandomForestRegressor on cross validation: %0.4f ( %0.2f)\" % (scores_reg_val.mean(), scores_reg_val.std()))\n",
    "    score_reg_test = reg.score(X_test, Y_test)\n",
    "    print('Accuracy of RandomForestRegressor on test data: {:.4f}'.format(score_reg_test))\n",
    "\n",
    "    # MSE\n",
    "    Y_hat = reg.predict(X_test)\n",
    "    mse = np.mean((Y_test - Y_hat)**2)\n",
    "    print(f'MSE for training data {mse}')\n",
    "\n",
    "    ax = subplots(figsize=(8,8))[1]\n",
    "    ax.scatter(Y_hat, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the training data, test data and the model\n",
    "def model_run(region_train, region_test, objective='Biome_obs', objective_list=None, \\\n",
    "                drop_columns=[], use_columns=None, \\\n",
    "                hyperparameter_tuning=False, continuous_Y=False):\n",
    "    X_train, X_test, Y_train, Y_test, feature_names = model_create_XY(region_train, \\\n",
    "                        region_test, drop_columns, use_columns, objective, objective_list, continuous_Y)\n",
    "    if continuous_Y:\n",
    "        regression_train_evaluate(X_train, X_test, Y_train, Y_test, feature_names, \\\n",
    "                         hyperparameter_tuning)\n",
    "    else:\n",
    "        model_train_evaluate(X_train, X_test, Y_train, Y_test, feature_names, \\\n",
    "                         hyperparameter_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_3, region_test_3, objective_list=biome_list_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some evaluation of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now run the model whilst removing the medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_median = [feature_name for feature_name in list(data_index_2) if feature_name[-6:]=='Median']\n",
    "print(f'We dropped the features: {features_median}')\n",
    "\n",
    "model_run(region_train_3, region_test_3, objective_list=biome_list_3, drop_columns=features_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how important the weather is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_weather = [feature_name for feature_name in list(data_index_2) if \\\n",
    "                   re.search('Fall|Summer|Winter|Spring',feature_name)]\n",
    "print(f'We dropped the features: {features_weather}')\n",
    "display(Markdown('---'))\n",
    "\n",
    "model_run(region_train_3, region_test_3, objective_list=biome_list_3, drop_columns=features_weather.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently not important at all. Now lets see if the model works only with weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We only use the features: {features_weather}')\n",
    "display(Markdown('---'))\n",
    "\n",
    "model_run(region_train_3, region_test_3, objective_list=biome_list_3, use_columns=features_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As regions we chose:\n",
    "- China for training\n",
    "- Africa for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_train_4 = data_index_2['Pan_2007'] == 'China'\n",
    "region_test_4 = data_index_2['Pan_2007'] == 'Africa'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, hyperparameter_tuning=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is relatively abysmal performance. Fiddling with the hyperparameters changes a lot though the\n",
    "hyperparameter optimisation is far from optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, \\\n",
    "          hyperparameter_tuning=True, drop_columns=features_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we remove the entire weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, \\\n",
    "          hyperparameter_tuning=True, drop_columns=features_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, our performance just improved. Let's see what happens if we use only weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_weather)\n",
    "model_run(region_train_4, region_test_4, \\\n",
    "          hyperparameter_tuning=True, use_columns=features_weather.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out the weather really does not play an important role. Let's see if the model works better at classifying 'Biome_Cmax'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, objective='Biome_Cmax', hyperparameter_tuning=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare with LPJ-Guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we creatively use the same test and validation set as in part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_train_5 = region_train_4\n",
    "region_test_5 = region_test_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_5, region_test_5, objective='NPP', continuous_Y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
