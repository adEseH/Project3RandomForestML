{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some libraries and the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "import sklearn.model_selection as skm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.experimental import enable_halving_search_cv # because it is experimental we need this\n",
    "from sklearn.model_selection import GridSearchCV, HalvingGridSearchCV\n",
    "from ISLP import load_data, confusion_table\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, log_loss, mean_squared_error,confusion_matrix, classification_report,balanced_accuracy_score, max_error, PredictionErrorDisplay,mean_absolute_error, mean_absolute_percentage_error)\n",
    "from sklearn.ensemble import (RandomForestRegressor, RandomForestClassifier)\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image, Markdown\n",
    "import graphviz\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statistics\n",
    "import re\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import spearmanr\n",
    "from collections import defaultdict\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no idea what this is\n",
    "# main dataset\n",
    "data_index_2 = pd.read_csv('../Project_datasets/data_index_2.csv', quotechar=\"'\")\n",
    "data_index_2 = data_index_2.drop(columns=['Unnamed: 0'])\n",
    "nan_template = ~(data_index_2.isna().any(axis=1))\n",
    "data_index_2 = data_index_2[nan_template] # delete rows with NaN\n",
    "# precipitation by day and statistics\n",
    "if True:\n",
    "    Predaymean = pd.read_csv('../Project_datasets/Predaymean1961_1990.csv')[nan_template]\n",
    "    Predaymean_statistics = pd.read_csv('../Project_datasets/Predaymean1961_1990_statics.csv')[nan_template]\n",
    "    # maximum temperature by day and statistics\n",
    "    Tmaxdaymean = pd.read_csv('../Project_datasets/Tmaxdaymean1961_1990.csv')[nan_template]\n",
    "    Tmaxdaymean_statistics = pd.read_csv('../Project_datasets/Tmaxdaymean1961_1990_statics.csv')[nan_template]\n",
    "    # minimum temperature by day and statistics\n",
    "    Tmindaymean = pd.read_csv('../Project_datasets/Tmindaymean1961_1990.csv')[nan_template]\n",
    "    Tmindaymean_statistics = pd.read_csv('../Project_datasets/Tmindaymean1961_1990_statics.csv')[nan_template]\n",
    "    # mean temperature by day and statistics\n",
    "    Tmpdaymean = pd.read_csv('../Project_datasets/Tmpdaymean1961_1990.csv')[nan_template]\n",
    "    Tmpdaymean_statistics = pd.read_csv('../Project_datasets/Tmpdaymean1961_1990_statics.csv')[nan_template]\n",
    "    # shortwave radiation flux\n",
    "    Tswrfdaymean = pd.read_csv('../Project_datasets/Tswrfdaymean1961_1990.csv')[nan_template]\n",
    "    Tswrfdaymean_statistics = pd.read_csv('../Project_datasets/Tswrfdaymean1961_1990_statics.csv')[nan_template]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridlist_pan_gfed_ISO3_UN = open('../Project_datasets/gridlist_pan_gfed_ISO3_UN.txt','r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the first entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short legend about the colums: \n",
    "\n",
    "Lon: Longitude<br>\n",
    "Lat: Latidude\n",
    "\n",
    "stuff about the souil texture: <br>\n",
    "\n",
    "clay: Clay in %<br>\n",
    "silt: Silt in % (sedimentary deposit that is formed when a river deposits the material it is carrying)<br>\n",
    "slay: slay in %(soil in eg. rainforest) <br>\n",
    "sand: sand in % %<br>\n",
    "orC: organic carbondioxid in %<br>\n",
    "CN: Cyanide in %<br>\n",
    "pH: pH<br>\n",
    "cell fraction: portion of cells / organic material in soil in % <br>\n",
    "\n",
    "\n",
    "Allways with season: Same as the tables below.\n",
    "\n",
    "tmax: maximum temperatur in K <br>\n",
    "tmin: min temperature in K <br>\n",
    "tmp:mean temperature in K <br>\n",
    "Pre: Precipitation, mm day-1 <br>\n",
    "tswrf: Total shortwave radiation flux, W m-2\n",
    "\n",
    "\n",
    "This is the GUESS output: \n",
    "\n",
    "NPP: net primary productivity (kg C m-2 year-1)<br>\n",
    "SoilR: soil respiration (kg C m-2 year-1)<br>\n",
    "MaxBiomeCmass: The maximum biomass from a single biome (kg C m-2)<br>\n",
    "MxbiomeLAI: The maximum leaf area index from a single biome (unitless)<br>\n",
    "VegC: Vegetation carbon poo (kg C m-2)l<br>\n",
    "LitterC: Litter carbon pool (kg C m-2)<br>\n",
    "SoilC: Soil carbon pool (kg C m-2)<br>\n",
    "Biome_Cmass: The biome type based on the maximum biomass (category)<br>\n",
    "Biome_LAI: The biome type based on the maximum LAI (category)<br>\n",
    "Biome_obs: The observed biome type (category)<br>\n",
    "\n",
    "Country codes\n",
    "\n",
    "GFED-region: Global Fire Emissions Database (https://www.un-spider.org/global-fire-emissions-database-gfed) <br>\n",
    "Pan_2007: Big reogion (Europa, Africa, Australia, USA, Russia, China, .... )<br>\n",
    "ISO3: Abbreviation for country <br>\n",
    "UN: Country code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {data_index_2.shape}')\n",
    "data_index_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Project_datasets/Readme.txt\",'r') as script:\n",
    "    speech = script.read().splitlines()\n",
    "\n",
    "count = 1\n",
    "for line in speech:\n",
    "    if False:\n",
    "        count+=1\n",
    "        if count % 2 == 0: #this is the remainder operator\n",
    "            print(line)\n",
    "    else:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pred (Precipitation, mm day-1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Predaymean.shape}')\n",
    "#Predaymean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Predaymean_statistics.shape}')\n",
    "#Predaymean_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tmp (Daily mean temperature, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmpdaymean.shape}')\n",
    "#Tmpdaymean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmpdaymean_statistics.shape}')\n",
    "#Tmpdaymean_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmaxdaymean.shape}')\n",
    "#Tmaxdaymean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmaxdaymean_statistics.shape}')\n",
    "#Tmaxdaymean_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmindaymean.shape}')\n",
    "#Tmindaymean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tmindaymean_statistics.shape}')\n",
    "#Tmindaymean_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tswrf (Total shortwave radiation flux, W m-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {Tswrfdaymean.shape}')\n",
    "#Tswrfdaymean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'shape: {Tswrfdaymean_statistics.shape}')\n",
    "#Tswrfdaymean_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### County List and binome legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from file into a list-of-lists table.\n",
    "with open('../Project_datasets/gridlist_pan_gfed_ISO3_UN.txt') as file:\n",
    "    datatable = [line.split() for line in file.read().splitlines()]\n",
    "\n",
    "country_codes = pd.DataFrame(datatable, columns = ['Lon', 'Lat', 'GFED-region', 'Pan_2007', 'ISO3', 'UN'] )  \n",
    "country_codes = country_codes.drop(index = 0)\n",
    "country_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "with open(\"../Project_datasets/legend of biomes.txt\",'r') as script:\n",
    "    speech = script.read().splitlines()\n",
    "\n",
    "count = 1\n",
    "for line in speech:\n",
    "    if True:\n",
    "        count+=1\n",
    "        if count % 2 == 0: #this is the remainder operator\n",
    "            print(line)\n",
    "    else:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of samples of different biomes in different coutries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countrys_sample_size = data_index_2.groupby(['ISO3', 'Biome_obs']).size()   \n",
    "#countrys_sample_size = pd.DataFrame(countrys_sample_size)\n",
    "#countrys_sample_size.columns = [ \"Size\"]           # Maybe solve later\n",
    "countrys_sample_size.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samply size for countries\n",
    "countrys_sample_size.at[\"EGY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and table generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names my have to be improved\n",
    "def df_to_latex(df, filename = 'mytable', caption=''):  \n",
    "    s = df.style.to_latex(\n",
    "    # column_format=\"rrrrr\",#  position=\"h\", position_float=\"centering\",\n",
    "    hrules=True,\n",
    "    multirow_align=\"t\", multicol_align=\"r\"\n",
    "    )  \n",
    "        \n",
    "    with open('../table/' + filename + '.tex', 'w') as f:\n",
    "        f.write(s)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(figure, figurename = 'my_plot'):\n",
    "    figure.savefig('../plots/' + figurename + '.pdf', bbox_inches='tight')\n",
    "\n",
    "## test\n",
    "#save_plot(fig, figurename ='worldmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biome_data_file = '../Project_datasets/legend of biomes.txt'\n",
    "\n",
    "biome_colors = {}\n",
    "biome_names = {}\n",
    "\n",
    "def parse_rgb_value(value):\n",
    "    # Check if the value starts with a dot and add a leading zero if needed\n",
    "    if value.startswith('.'):\n",
    "        value = '0' + value\n",
    "    return float(value)\n",
    "\n",
    "with open(biome_data_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        \n",
    "        biome_number, biome_name = lines[i].strip().split(maxsplit=1)\n",
    "        \n",
    "        rgb_values = list(map(parse_rgb_value, lines[i + 1].strip().split()))\n",
    "        #rgb_tuple = tuple(int(val * 255) for val in rgb_values)   # no need to do this.....indexerror will arise\n",
    "        \n",
    "        # Add data to dictionaries\n",
    "        biome_colors[int(biome_number)] = rgb_values\n",
    "        biome_names[int(biome_number)] = biome_name\n",
    "        \n",
    "        # Move to the next biome data\n",
    "        i += 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_region(region=None, Y_hat=None, parameter='Biome_obs', plot_name=None, continuous_Y=False):\n",
    "    \"\"\"\n",
    "    region: bolean list of regions to be plotted\n",
    "    Y_hat: values to compare to\n",
    "    \"\"\"\n",
    "    if not np.any(region): region = np.ones((data_index_2.shape[0],),dtype=bool)\n",
    "    if not np.any(Y_hat): Y_hat=np.asarray(data_index_2.loc[region][parameter])\n",
    "    # check if the region is russia\n",
    "    if ((data_index_2['ISO3'] == 'RUS')==region).all():\n",
    "        # cut off that bit at alaska\n",
    "        Y_hat = Y_hat[data_index_2.loc[region]['Lon']>-50]\n",
    "        region = region & (data_index_2['Lon'] >-50)\n",
    "    Y_hat = pd.DataFrame(Y_hat, index=data_index_2.loc[region].index, columns=[parameter])\n",
    "    lon_lat={}\n",
    "    for i,sel in enumerate(['Lon', 'Lat']):\n",
    "        lon_lat[i] = data_index_2.loc[region][sel]\n",
    "\n",
    "\n",
    "    biome = data_index_2[parameter]\n",
    "\n",
    "    width_ratio=(max(lon_lat[1])-min(lon_lat[1]))/(max(lon_lat[0])-min(lon_lat[0]))\n",
    "    fig, ax = plt.subplots(figsize=(10,10),dpi=300)\n",
    "    \n",
    "    # Now we crop the world to focus on the locations whose biome was predicted wrongly\n",
    "    longer_side=0\n",
    "    for i,func in enumerate([ax.set_xlim, ax.set_ylim]):\n",
    "        max_lon_lat = max(lon_lat[i])\n",
    "        min_lon_lat = min(lon_lat[i])\n",
    "        func((min_lon_lat-2,max_lon_lat+2)) #crop image\n",
    "        longer_side=max(longer_side,max_lon_lat-min_lon_lat)\n",
    "\n",
    "\n",
    "    geometry = [Point(lon, lat) for lon, lat in zip(lon_lat[0], lon_lat[1])]\n",
    "    gdf = gpd.GeoDataFrame(data_index_2.loc[region][parameter], geometry=geometry)\n",
    "    gdf_Y_hat = gpd.GeoDataFrame(Y_hat[parameter], geometry=geometry)\n",
    "\n",
    "    markersize = 50000/(longer_side**2)\n",
    "    if not continuous_Y:\n",
    "        gdf_err = gdf[gdf_Y_hat[parameter]!=gdf[parameter]]\n",
    "        gdf_err.plot(ax=ax, color='black', label='wrongly classified', marker='s',markersize=markersize)\n",
    "        # Plot points with different colors based on biome\n",
    "        for b in set(biome): \n",
    "            gdf_b = gdf[(gdf[parameter] == b) & (gdf_Y_hat[parameter]==gdf[parameter])]\n",
    "            gdf_b.plot(ax=ax, color=biome_colors[b], label=biome_names[b],marker='s', markersize=markersize)\n",
    "\n",
    "        # fig.set_size_inches(2.5/width_ratio,2.5)\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), markerscale=((longer_side)/30))\n",
    "        # 50000/(longer_side**2) if longer_side > 100 else 500/))\n",
    "    else:\n",
    "        gdf.plot(ax=ax, column=parameter, marker='s', markersize=markersize, legend=True, \\\n",
    "                legend_kwds={\"label\": f\"difference in {parameter}\"})\n",
    "    \n",
    "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    ax = world.plot(ax=ax, color=(0,0,0,0), edgecolor=(0,0,0,1))\n",
    "    \n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    # Add legend\n",
    "    #plt.legend()\n",
    "    # Add a title\n",
    "    plt.title('Biome Map')\n",
    "\n",
    "    if plot_name: plt.savefig(f'../plots/{plot_name}_map.pdf', dpi=300, bbox_inches='tight')\n",
    "    # Show the map\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_region(region_test_3) # plot the world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot function for statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_statistics(data, name_data = 'data'):\n",
    "    \n",
    "    obs_data = \"Biome_obs\"\n",
    "    fig, ax = plt.subplots(figsize = (8,4))\n",
    "    ax = sns.countplot(data, x = obs_data)\n",
    "    save_plot(fig, figurename = 'countplot_' + name_data + '_' + obs_data)\n",
    "    \n",
    "    obs_data = \"Biome_Cmax\"\n",
    "    fig, ax = plt.subplots(figsize = (8,4))\n",
    "    ax = sns.countplot(data, x = obs_data)\n",
    "    save_plot(fig, figurename = 'countplot_' + name_data + '_' + obs_data)\n",
    "    \n",
    "    ## sample biomes\n",
    "    obs_data = \"Biome_obs_Pan_2007\"\n",
    "    fig, ax = plt.subplots(figsize = (8,4))\n",
    "    ax = sns.countplot(data, x = 'Pan_2007', hue = 'Biome_obs')\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    save_plot(fig, figurename = 'countplot_' + name_data + '_' + obs_data)\n",
    "    \n",
    "    ## sample biomes\n",
    "    obs_data = \"Biome_obs_ISO3\"\n",
    "    fig, ax = plt.subplots(figsize = (8,4))\n",
    "    ax = sns.countplot(data, x = 'ISO3', hue = 'Biome_obs')\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    save_plot(fig, figurename = 'countplot_' + name_data + '_' + obs_data)\n",
    "    \n",
    "    ## scatterplot biome_Cmax and biome_LAI\n",
    "    obs_data = \"Biome_obs_Biome_Cmax\"\n",
    "    fig, ax = plt.subplots(figsize = (4,4))\n",
    "    ax = sns.scatterplot(data, x = \"Biome_Cmax\", y = \"Biome_obs\")\n",
    "    ax.set_yticks(range(1, 20,2))\n",
    "    save_plot(fig, figurename = 'scatterplot_' + name_data + '_' + obs_data)\n",
    "    \n",
    "    '''\n",
    "    obs_data = \"Biome_obs_Biome_LAI\"\n",
    "    fig, ax = plt.subplots(figsize = (4,4))\n",
    "    ax = sns.scatterplot(data, x = \"Biome_LAI\", y = \"Biome_obs\")\n",
    "    ax.set_yticks(range(1, 20,2))\n",
    "    save_plot(fig, figurename = 'scatterplot_' + name_data + '_' + obs_data)\n",
    "    '''\n",
    "    \n",
    "    # NPP\n",
    "    obs_data = \"NPP\"\n",
    "    fig, axs = plt.subplots( 1,2, constrained_layout=True)\n",
    "    sns.histplot(data, x=obs_data,  kde=True, ax=axs[0]) #stat=\"density\", kde=True,, log_scale=True\n",
    "    sns.ecdfplot(data, x=\"NPP\", ax=axs[1])\n",
    "    save_plot(fig, figurename = 'histogramm_' + name_data + '_' + obs_data)\n",
    "    \n",
    "    #\"VegC\"\n",
    "    obs_data = \"VegC\"\n",
    "    fig, axs = plt.subplots(1,2, constrained_layout=True)\n",
    "    sns.histplot(data, x=obs_data,   kde=True, ax=axs[0]) #stat=\"density\", kde=True,binwidth=1,\n",
    "    sns.ecdfplot(data, x=\"VegC\", ax=axs[1])\n",
    "    save_plot(fig, figurename = 'histogramm_' + name_data + '_' + obs_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_params = ['pre','tmp|tmin|tmax','tswrf']\n",
    "\n",
    "features_weather = [feature_name for feature_name in list(data_index_2) if \\\n",
    "                   re.search('Fall|Summer|Winter|Spring',feature_name)]\n",
    "\n",
    "liste = ['Lat', 'Lon', 'UN', 'GFED-region', 'Biome_obs', 'Biome_LAI', 'Biome_Cmax']+features_weather\n",
    "\n",
    "print(data_index_2.head())\n",
    "print(data_index_2.drop(liste, axis=1).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countplot to count the number of data for each biom: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.scatterplot(data_index_2, x=\"Lon\", y=\"Lat\", hue=\"Biome_obs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_statistics(data_index_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biome with index 4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index_2.loc[data_index_2['Biome_obs'] == 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Classifiaction and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function generates X_test, Y_test and X_train, Y_train for the binary classification\n",
    "def model_create_XY(region_train, region_test, \\\n",
    "                     drop_columns=[], use_columns=None, \\\n",
    "                        objective='Biome_obs', objective_list=None, continuous_Y=False,\n",
    "                        objective_test=None, drop_LPG_guess=True):\n",
    "      \n",
    "      drop_columns = drop_columns.copy() + [objective] # we would not want our objective to appear in the training data\n",
    "\n",
    "      # restrict to two biomes, restrict to country codes, remove any rows with NaN\n",
    "      if not continuous_Y:\n",
    "            if not objective_list: objective_list=list(data_index_2.loc[region_train][objective].drop_duplicates())\n",
    "            relevant_data_train = data_index_2.loc[data_index_2[objective].isin(objective_list)] # restrict to given biomes\n",
    "            # in case we want to use LPJ_guess output, write LPJ_ before the objective_list\n",
    "            relevant_data_test = data_index_2.loc[data_index_2[objective_test].isin(objective_list)] # restrict to given biomes\n",
    "      else:\n",
    "            relevant_data_train = data_index_2\n",
    "            relevant_data_test = data_index_2\n",
    "\n",
    "                  \n",
    "      relevant_data_train = relevant_data_train.loc[region_train] # restrict to training country\n",
    "      relevant_data_test = relevant_data_test.loc[region_test] # restrict to test country\n",
    "\n",
    "      drop_columns += ['MaxBiomeLAI','Biome_obs','Biome_LAI','Biome_Cmax',\n",
    "                       'Lon','Lat','Pan_2007','ISO3','UN','MaxBiomeCmax']\n",
    "      if drop_LPG_guess: drop_columns += ['CN','pH','cellfraction','NPP','VegC','SoilC','LitterC','SoilR','GFED-region'] # drop these columns\n",
    "      if use_columns:\n",
    "            X_train = relevant_data_train[use_columns]\n",
    "            X_test = relevant_data_test[use_columns]\n",
    "      else:\n",
    "            X_train = relevant_data_train.drop(columns=drop_columns)\n",
    "            X_test = relevant_data_test.drop(columns=drop_columns)\n",
    "\n",
    "      feature_names = list(X_train.columns)\n",
    "\n",
    "      Y_train = relevant_data_train[objective]\n",
    "      Y_test = relevant_data_test[objective_test]\n",
    "\n",
    "      print(f\"length of training data: {Y_train.shape[0]}\")\n",
    "      print(f\"length of testing data: {Y_test.shape[0]}\")\n",
    "      \n",
    "\n",
    "      return X_train, X_test, Y_train, Y_test, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Feature importance (Permutation importance vs. purity importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Notes\n",
    "- The more accurate model is, the more trustworthy computed importances are.\n",
    "- The computed importances describe how important features are for the machine learning model. It is an approximation of how important features are in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_permutation_importance(clf, X, y, ax):\n",
    "    result = permutation_importance(clf, X, y,  n_repeats=10, random_state=42, n_jobs=2) # n_repeats=10, random_state=42, n_jobs=2\n",
    "    perm_sorted_idx = result.importances_mean.argsort()\n",
    "    \n",
    "    x_val = X.columns[perm_sorted_idx]\n",
    "    y_val = result.importances[perm_sorted_idx].mean(axis=1).T\n",
    "    \n",
    "    if len(y_val) > 10:\n",
    "        x_val = x_val[:5].union(x_val[-5:])\n",
    "        y_val = np.concatenate([y_val[:5],y_val[-5:]])\n",
    "    ax.barh(x_val, y_val)\n",
    "    ax.tick_params(axis='y', which = 'minor', labelsize=10)\n",
    "    #ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "    return ax\n",
    "\n",
    "def plot_permutation_importance_big_graph(clf, X_train, X_test,  Y_train, Y_test, experiment_name):\n",
    "\n",
    "    mdi_importances = pd.Series(clf.feature_importances_, index=X_train.columns)\n",
    "    # tree_importance_sorted_idx = np.argsort(clf.feature_importances_)\n",
    "    # tree_indices = np.arange(0, len(clf.feature_importances_)) + 0.5\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(9, 4))\n",
    "    y = mdi_importances.sort_values()\n",
    "    y = pd.concat([y[:5],y[-5:]])\n",
    "    y.plot.barh(ax=ax1)\n",
    "    ax1.set_xlabel(\"Gini importance\")\n",
    "    ax1.set_title('MDI importance')\n",
    "    plot_permutation_importance(clf, X_train, Y_train, ax2)\n",
    "    ax2.set_xlabel(\"Decrease in accuracy \")\n",
    "    ax2.set_title('permutation importance (train)')\n",
    "    #fig.suptitle(\n",
    "     #   \"Impurity-based vs. permutation importances on multicollinear features\"\n",
    "    #)\n",
    "    plot_permutation_importance(clf, X_test, Y_test, ax3)\n",
    "    ax3.set_xlabel(\"Decrease in accuracy\")\n",
    "    ax3.set_title('permutation importance (test)')\n",
    "    _ = fig.tight_layout()\n",
    "    save_plot(fig, figurename = 'histogramm_feature_imp'+ experiment_name )\n",
    "    return fig\n",
    "\n",
    "def clustering(clf, X_train, X_test,  Y_train, Y_test, experiment_name, continuous_Y=False):\n",
    "    \n",
    "    \n",
    "    fig, (ax1) = plt.subplots( figsize=(6,7))\n",
    "    corr = spearmanr(X_train).correlation\n",
    "\n",
    "    # Ensure the correlation matrix is symmetric\n",
    "    corr = (corr + corr.T) / 2\n",
    "    np.fill_diagonal(corr, 1)\n",
    "\n",
    "    # We convert the correlation matrix to a distance matrix before performing\n",
    "    # hierarchical clustering using Ward's linkage.\n",
    "    distance_matrix = 1 - np.abs(corr)\n",
    "    dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "    dendro = hierarchy.dendrogram(\n",
    "        dist_linkage, labels=X_train.columns.to_list(), ax=ax1, leaf_rotation=90\n",
    "    )\n",
    "\n",
    "    save_plot(fig, figurename = 'dendogramm_feature_imp'+ experiment_name )\n",
    "    dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
    "    \n",
    "    cluster_ids = hierarchy.fcluster(dist_linkage, 1, criterion=\"distance\")\n",
    "    cluster_id_to_feature_ids = defaultdict(list)\n",
    "    for idx, cluster_id in enumerate(cluster_ids):\n",
    "        cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "    selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "    selected_features_names = X_train.columns[selected_features]\n",
    "    selected_features_names = selected_features_names.tolist()\n",
    "    \n",
    "    X_train_sel = X_train[selected_features_names]\n",
    "    X_test_sel = X_test[selected_features_names]\n",
    "\n",
    "    clf_sel = clf\n",
    "\n",
    "    clf_sel.fit(X_train_sel, Y_train)\n",
    "    if not continuous_Y:\n",
    "        print(\n",
    "            \"Baseline accuracy on test data with features removed:\"\n",
    "            f\" {accuracy_score(Y_test, clf_sel.predict(X_test_sel)):.4}\"\n",
    "            \"Baseline balanced accuracy on test data with features removed:\"\n",
    "            f\" {balanced_accuracy_score(Y_test, clf_sel.predict(X_test_sel)):.4}\"\n",
    "        )\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    plot_permutation_importance(clf_sel, X_test_sel, Y_test, ax)\n",
    "    #ax.set_title(\"Permutation Importances on selected subset of features\\n(test set)\")\n",
    "    ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "    ax.figure.tight_layout()\n",
    "    save_plot(fig, figurename = 'histogramm_feature_imp_clustered'+ experiment_name )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following generates, trains and evaluates the model\n",
    "def model_train_evaluate(X_train, X_test, Y_train, Y_test, feature_names, \\\n",
    "                         hyperparameter_tuning=False, feature_plots=False, experiment_name=None,\n",
    "                         plotmap=False, objective_test=None):\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=0)\n",
    "    \n",
    "    if hyperparameter_tuning==4: # for testing parameters manually\n",
    "        clf = RandomForestClassifier(random_state=0, max_depth=10, n_estimators=100, min_samples_leaf=1)\n",
    "    elif hyperparameter_tuning:\n",
    "        X_hyper, Y_hyper = X_train, Y_train\n",
    "        # X_hyper,_,Y_hyper,_ = skm.train_test_split(X_train, Y_train, train_size=300,random_state=0)\n",
    "        kfold3 = skm.KFold(3,\n",
    "                        random_state=1,\n",
    "                        shuffle=True) # let's try fewer fold to save running time\n",
    "\n",
    "        fineness = 10 # determines how many parameters should be tested\n",
    "        params = {\n",
    "                'max_depth': np.linspace(5, 15, 1).astype('int'), # (5,15)\n",
    "                'n_estimators': np.linspace(50, 150, fineness).astype('int'), #50, 150 (10,40) CV number of trees, keep this low for part 4\n",
    "                'min_samples_leaf': np.linspace(1, 5, fineness).astype('int'), # minimum leaf number\n",
    "                }\n",
    "        if False:\n",
    "            rfc_gscv = GridSearchCV(clf, param_grid = params, scoring = \"accuracy\",\n",
    "                                        cv = kfold3 )\n",
    "        else:\n",
    "            # Alternatively use HalvingGridSearchCV which is equally abismal in its performance\n",
    "            rfc_gscv = HalvingGridSearchCV(clf, param_grid = params, scoring = \"accuracy\",\n",
    "                                        cv = kfold3, min_resources=20, max_resources=30)\n",
    "        \n",
    "        # Fit the model\n",
    "        model_rfc = rfc_gscv.fit(X_hyper, Y_hyper)\n",
    "\n",
    "        # Model best estimator\n",
    "        max_depths=model_rfc.best_estimator_.get_params()[\"max_depth\"]\n",
    "        max_trees= model_rfc.best_estimator_.get_params()[\"n_estimators\"]\n",
    "        min_samples_leaf= model_rfc.best_estimator_.get_params()[\"min_samples_leaf\"]\n",
    "        max_cvs= rfc_gscv.best_score_\n",
    "        print(\"Max Depth: \", max_depths)\n",
    "        print(\"Max Trees: \",max_trees)\n",
    "        print(\"Min Leafs: \",min_samples_leaf)\n",
    "        print(\"Max CV: \",max_cvs)\n",
    "\n",
    "        clf = RandomForestClassifier(random_state=0, max_depth=max_depths, n_estimators=max_trees, min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "    # Some performance \n",
    "    score_rfc_train = accuracy_score(Y_train, clf.predict(X_train))\n",
    "    print('Accuracy of RandomForestClassifier on train data: {:.4f}'.format(score_rfc_train))\n",
    "    balance_score_rfc_train = balanced_accuracy_score(Y_train, clf.predict(X_train))\n",
    "    print('Balanced accuracy of RandomForestClassifier on train data: {:.4f}'.format(balance_score_rfc_train))\n",
    "    \n",
    "    kfold = skm.KFold(3, random_state=1, shuffle=True)\n",
    "    # This becomes computationally quite expensive for large training sets\n",
    "    scores_rfc_val = cross_val_score(clf, X_train, Y_train, cv=kfold)\n",
    "    print(\"Accuracy RandomForestClassifier on cross validation: %0.4f ( %0.2f)\" % (scores_rfc_val.mean(), scores_rfc_val.std()))\n",
    "\n",
    "    score_rfc_test = accuracy_score(Y_test, clf.predict(X_test))\n",
    "    print('Accuracy of RandomForestClassifier on test data: {:.4f}'.format(score_rfc_test))\n",
    "    balance_score_rfc_test = balanced_accuracy_score(Y_test, clf.predict(X_test))\n",
    "    print('Balanced accuracy of RandomForestClassifier on train data: {:.4f}'.format(balance_score_rfc_test))\n",
    "    \n",
    "    # confusion table\n",
    "    display(Markdown('---\\n The confusion table'))\n",
    "    conf_table = confusion_table(clf.predict(X_test),\n",
    "                            Y_test)\n",
    "    display(conf_table)\n",
    "    if experiment_name:\n",
    "        df_to_latex(conf_table, f'{experiment_name}_confTable', caption='Confusion matrix.')\n",
    "    \n",
    "    Y_hat = clf.predict(X_test)\n",
    "    classreport = classification_report(Y_test, Y_hat, zero_division = np.nan, output_dict=True)\n",
    "    classreport = pd.DataFrame(classreport).transpose()\n",
    "    display(classreport)\n",
    "    if experiment_name:\n",
    "        df_to_latex(classreport, f'{experiment_name}_classreport')\n",
    "    \n",
    "    # display importance table\n",
    "    display(Markdown('---\\n The feature importance'))\n",
    "    feature_imp = pd.DataFrame(\n",
    "    {'importance':clf.feature_importances_},\n",
    "    index=feature_names)\n",
    "    feature_imp = feature_imp.sort_values(by='importance', ascending=False)\n",
    "    display(feature_imp)\n",
    "    if experiment_name:\n",
    "        df_to_latex(feature_imp, f'{experiment_name}_featureImportance')\n",
    "    \n",
    "\n",
    "    if plotmap:\n",
    "        region_test = np.zeros((data_index_2.shape[0],),dtype=bool)\n",
    "        region_test[np.asarray(X_test.reset_index()['index'])]=True\n",
    "        plot_region(region_test, Y_hat, objective_test, f'{experiment_name}_error')\n",
    "    \n",
    "    if feature_plots:\n",
    "        # Plot Impurity-based vs. Permutation importance\n",
    "        plot_permutation_importance_big_graph(clf, X_train, X_test,  Y_train, Y_test, experiment_name)\n",
    "        \n",
    "        clustering(clf, X_train, X_test,  Y_train, Y_test,  experiment_name)\n",
    "        \n",
    "    display(Markdown('---'))\n",
    "\n",
    "    # return in the format\n",
    "    # ['accuracy, train', 'balanced accuracy, train', 'cross val accuracy, train',\n",
    "    #  'accuracy, test', 'balanced accuracy, test']\n",
    "\n",
    "    return [score_rfc_train ,balance_score_rfc_train, scores_rfc_val, \n",
    "            score_rfc_test, balance_score_rfc_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_train_evaluate(X_train, X_test, Y_train, Y_test, feature_names, \\\n",
    "                         hyperparameter_tuning=False, feature_plots=False, experiment_name=None,\n",
    "                         plotmap=False, objective_test='Biome_obs'):\n",
    "    reg = RandomForestRegressor()\n",
    "    reg.fit(X_train, Y_train)\n",
    "\n",
    "    score_reg_train = reg.score(X_train, Y_train)\n",
    "    #Return the coefficient of determination of the prediction (R^2 score)\n",
    "    \n",
    "    print('R^2 Score of RandomForestRegressor on train data: {:.4f}'.format(score_reg_train))\n",
    "    # The following takes way to long for large training sets\n",
    "    # kfold = skm.KFold(3, random_state=1, shuffle=True)\n",
    "    # scores_reg_val = cross_val_score(reg, X_train, Y_train, cv=kfold)\n",
    "    # print(\"Accuracy RandomForestRegressor on cross validation: %0.4f ( %0.2f)\" % (scores_reg_val.mean(), scores_reg_val.std()))\n",
    "    score_reg_test = reg.score(X_test, Y_test)\n",
    "    print('R^2 Score of RandomForestRegressor on test data: {:.4f}'.format(score_reg_test))\n",
    "\n",
    "    # MSE\n",
    "    Y_hat = reg.predict(X_test)\n",
    "    mse = np.mean((Y_test - Y_hat)**2)\n",
    "    print(f'MSE for test data {mse}')\n",
    "\n",
    "    mse2 = mean_squared_error(Y_test,Y_hat)\n",
    "    print(f'MSE (with sktfct) for test  data {mse2}')\n",
    "\n",
    "    # SQRT(MSE)\n",
    "    sqrtmse = np.sqrt(mse)\n",
    "    print(f'sqrt(MSE) for test data {sqrtmse}')\n",
    "    \n",
    "    # maximum Error\n",
    "    max_err = max_error(Y_test, Y_hat)\n",
    "    print(f'max error for test data {max_err}')\n",
    "\n",
    "    # mean absolute error\n",
    "    mean_abs_err = mean_absolute_error(Y_test, Y_hat)\n",
    "    print(f'mean abs error for test data {mean_abs_err}')\n",
    "\n",
    "    #PredictionErrorDisplay(Y_test, Y_hat)\n",
    "\n",
    "    # fig, ax = subplots()\n",
    "    plt_data = pd.DataFrame(np.concatenate([[Y_hat],[Y_test]])).transpose()\n",
    "    plt_data = plt_data.rename(columns={0:'Y_hat',1:'Y_test'})\n",
    "    # ax.scatter(Y_hat, Y_test)\n",
    "    ax = sns.relplot(data=plt_data,x='Y_hat',y='Y_test').ax\n",
    "    ax.set_xlabel('predicted sample $\\hat{Y}$')\n",
    "    ax.set_ylabel('exact sample $Y_{test}$')\n",
    "    maxval = max([np.max(Y_test),np.max(Y_hat)])\n",
    "    ax.plot([0,maxval],[0,maxval],color='red',markersize=1)\n",
    "    if experiment_name: save_plot(plt.gcf(), f'{experiment_name}_regressionPlot')\n",
    "\n",
    "    # fig, ax = plt.subplots()\n",
    "    # ax.hist(Y_test-Y_hat, bins=50, density=True)\n",
    "    ax = sns.displot(Y_test-Y_hat, kde=True).ax\n",
    "    ax.set_xlabel('residue $Y_{test}-\\hat{Y}$')\n",
    "    ax.set_ylabel('Number of samples')\n",
    "    if experiment_name: save_plot(plt.gcf(), f'{experiment_name}_residualDistr')\n",
    "\n",
    "    \n",
    "    if plotmap:\n",
    "        region_test = np.zeros((data_index_2.shape[0]+1,),dtype=bool)\n",
    "        region_test[np.asarray(X_test.reset_index()['index'])]=True\n",
    "        region_test = region_test[nan_template]\n",
    "        plot_region(region_test, Y_hat-Y_test, objective_test, f'{experiment_name}_error', True)\n",
    "    \n",
    "    # display importance table\n",
    "    display(Markdown('---\\n The feature importance'))\n",
    "    feature_imp = pd.DataFrame(\n",
    "    {'importance':reg.feature_importances_},\n",
    "    index=feature_names)\n",
    "    feature_imp = feature_imp.sort_values(by='importance', ascending=False)\n",
    "    display(feature_imp)\n",
    "    if experiment_name:\n",
    "        df_to_latex(feature_imp, f'{experiment_name}_featureImportance')\n",
    "\n",
    "    \n",
    "    if feature_plots:\n",
    "        # Plot Impurity-based vs. Permutation importance\n",
    "        plot_permutation_importance_big_graph(reg, X_train, X_test,  Y_train, Y_test, experiment_name)\n",
    "        \n",
    "        clustering(reg, X_train, X_test,  Y_train, Y_test,  experiment_name, True)\n",
    "    display(Markdown('---'))\n",
    "    # ['R^2 score, train', 'score, test', 'MSE, test', 'MSE sktfct, test', 'sqrt(MSE)', 'max err', 'mean abs err']\n",
    "    return [score_reg_train, score_reg_test, mse, mse2, sqrtmse, max_err, mean_abs_err]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about R2 score - which is the accuracz in the regression case: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score\n",
    "\n",
    "In cases where negative values arise, the mean of the data provides a better fit to the outcomes than do the fitted function values, according to this particular criterion. (Wikipedia: https://en.wikipedia.org/wiki/Coefficient_of_determination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model \n",
    "continuous_Y = True gives regression, else classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run the training data, test data and the model\n",
    "def model_run(region_train, region_test, objective='Biome_obs', objective_list=None, \\\n",
    "                drop_columns=[], use_columns=None, \\\n",
    "                hyperparameter_tuning=False, continuous_Y=False, objective_test=None, \\\n",
    "                    feature_plots=False, experiment_name=None, drop_LPG_guess=True, plotmap=False):\n",
    "    if not objective_test: objective_test = objective\n",
    "    X_train, X_test, Y_train, Y_test, feature_names = model_create_XY(region_train, \\\n",
    "                        region_test, drop_columns, use_columns, objective, \\\n",
    "                              objective_list, continuous_Y, objective_test, drop_LPG_guess)\n",
    "\n",
    "    if continuous_Y:\n",
    "        return regression_train_evaluate(X_train, X_test, Y_train, Y_test, feature_names, \\\n",
    "                         hyperparameter_tuning, feature_plots, experiment_name, plotmap, \\\n",
    "                            objective_test)\n",
    "    else:\n",
    "        return model_train_evaluate(X_train, X_test, Y_train, Y_test, feature_names, \\\n",
    "                         hyperparameter_tuning, feature_plots, experiment_name, plotmap,\n",
    "                         objective_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that `NaN` appeard somewhere in `data_index_2`. The following is to find out where it occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(data_index_2.loc[:,data_index_2.isna().any()]))\n",
    "data_index_2[data_index_2.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose biomes. Random.org gave us the biomes\n",
    "- 17: desert\n",
    "- 16: Arid shrub/steppe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biome_list_3 = [17,16] # our chosen biomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countplot: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_data = data_index_2.loc[data_index_2['Biome_obs'].isin(biome_list_3)]\n",
    "relevant_data = relevant_data[~(relevant_data.isna().any(axis=1))] # delete rows with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tmp = data_index_2\n",
    "tmp['count']=1\n",
    "tmp = tmp.groupby(['ISO3','Biome_obs']).sum().reset_index()\n",
    "candidates = set(tmp.loc[(tmp['Biome_obs']==16)&(tmp['count']>30)]['ISO3'])\n",
    "candidates = candidates.intersection(set(tmp.loc[(tmp['Biome_obs']==17)&(tmp['count']>30)]['ISO3']))\n",
    "tmp = tmp.loc[tmp['ISO3'].isin(candidates)]\n",
    "tmp = tmp.loc[tmp['Biome_obs'].isin([17,16])]\n",
    "tmp = tmp.loc[tmp['ISO3']!='[]']\n",
    "g = sns.catplot(data=tmp,col='ISO3',x='Biome_obs',y='count',kind='bar',aspect=0.15, height=4)\n",
    "\n",
    "g.set_axis_labels(\"\", \"Number of samples\")\n",
    "g.set_xticklabels([\"Shrub\", \"Desert\"], rotation=90)\n",
    "g.set_titles(\"{col_name}\")\n",
    "save_plot(plt.gcf(), 'shrub_desert_country_list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we choose Egypt to train and China as test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_train_3 = data_index_2['ISO3'] == 'EGY' # egypt is the chosen one\n",
    "region_test_3 = data_index_2['ISO3'] == 'LBY' # libya is the chosen one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "data1 = relevant_data.loc[region_train_3] \n",
    "data2 = relevant_data.loc[region_test_3]\n",
    "\n",
    "data = pd.concat([data1,data2])\n",
    "#plot_statistics(data, name_data = 'Section3EGY-CHN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_climate_diagramm(region, figname=None):\n",
    "    fig, ax1=plt.subplots()\n",
    "\n",
    "    tmp = Tmpdaymean.loc[region].transpose()[2:]\n",
    "    tmp.reset_index(inplace=True, drop=True)\n",
    "    tmp = tmp.stack(level=0).droplevel(1)\n",
    "    tmp_df = pd.DataFrame({'day':tmp.index, 'tmp':tmp.values})\n",
    "    tmp_df['tmp'] -= 273.15\n",
    "    p1 = sns.lineplot(ax=ax1, data=tmp_df, x=\"day\", y=\"tmp\", color='red')\n",
    "\n",
    "    ax2 =ax1.twinx()\n",
    "    tmp = Predaymean.loc[region].transpose()[2:]\n",
    "    tmp.reset_index(inplace=True, drop=True)\n",
    "    tmp = tmp.stack(level=0).droplevel(1)\n",
    "    tmp_df = pd.DataFrame({'day':tmp.index, 'prec':tmp.values})\n",
    "    p2 = sns.lineplot(ax=ax2, data=tmp_df, x=\"day\", y=\"prec\", color='blue')\n",
    "\n",
    "    ax3 = ax1.twinx()\n",
    "    tmp = Tswrfdaymean.loc[region].transpose()[2:]\n",
    "    tmp.reset_index(inplace=True, drop=True)\n",
    "    tmp = tmp.stack(level=0).droplevel(1)\n",
    "    tmp_df = pd.DataFrame({'day':tmp.index, 'tmp':tmp.values})\n",
    "    p3 = sns.lineplot(ax=ax3, data=tmp_df, x=\"day\", y=\"tmp\", color='orange')\n",
    "\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "\n",
    "    ax1.yaxis.label.set_color('red')\n",
    "    ax2.yaxis.label.set_color('blue')\n",
    "    ax3.yaxis.label.set_color('orange')\n",
    "\n",
    "    ax1.set_ylabel('Temperature (C)')\n",
    "    ax2.set_ylabel('Precipitation (mm) / day')\n",
    "    ax3.set_ylabel('Radiation ($W/m^2$) / day')\n",
    "\n",
    "    if figname: save_plot(plt.gcf(), f'climate_{figname}')\n",
    "\n",
    "    # fig, ax=plt.subplots()\n",
    "\n",
    "    # ax.set_ylabel('Radiation ($W/m^2$) / day')\n",
    "    # if figname: save_plot(plt.gcf(), f'radiation_{figname}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot climate diagramms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_climate_diagramm(region_test_3, 'egypt')\n",
    "plot_climate_diagramm(region_test_3 & (data_index_2['Biome_obs']==17), 'egypt_desert')\n",
    "plot_climate_diagramm(region_test_3 & (data_index_2['Biome_obs']==16), 'egypt_aridShrub')\n",
    "plot_climate_diagramm(region_train_3, 'libya')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_median = [feature_name for feature_name in list(data_index_2) if feature_name[-6:]=='Median']\n",
    "features_weather = [feature_name for feature_name in list(data_index_2) if \\\n",
    "                   re.search('Fall|Summer|Winter|Spring',feature_name)]\n",
    "seasons = ['Fall','Summer','Winter','Spring']\n",
    "features_seasons = [[feature_name for feature_name in list(data_index_2) if \\\n",
    "                   re.search(season,feature_name)] for season in seasons]\n",
    "weather_params = ['pre','tmp|tmin|tmax','tswrf']\n",
    "features_weather_params = [[feature_name for feature_name in list(data_index_2) if \\\n",
    "                   re.search(weather_param,feature_name)] for weather_param in weather_params]\n",
    "features_radiation = [feature_name for feature_name in list(data_index_2) if \\\n",
    "                   re.search('tswrf',feature_name) and not re.search('Median',feature_name)] # selected few radiation features\n",
    "\n",
    "features_nonweather = ['clay','silt','sand','orgC']\n",
    "\n",
    "features_names = ['median']+['soil']+['climate']+[season for season in seasons]+[weather_param for weather_param in weather_params]\n",
    "drop_features = [features_median]+[features_nonweather]+[features_weather]+[features_season for features_season in features_seasons]\\\n",
    "                +[features_weather_param for features_weather_param in features_weather_params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some more statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_distribution(show_labels, region_list, region_names, height=1.):\n",
    "    tmp=[]\n",
    "    for region in region_list:\n",
    "        tmp += [data_index_2.loc[region][show_labels].transpose().stack(level=0).droplevel(1)]\n",
    "    tmp_df = {}\n",
    "    num_plts = len(show_labels)\n",
    "    # fig, axs = plt.subplots(num_plts,1,squeeze=True, sharex=False, sharey=False,figsize=(8,12))\n",
    "    for i in range(len(tmp)):\n",
    "        tmp_df[i] = pd.DataFrame({'label':tmp[i].index, 'value':tmp[i].values, 'hue':i/len(tmp)})\n",
    "    tmp_df = pd.concat(tmp_df)\n",
    "    g = sns.FacetGrid(tmp_df, row=\"label\",height=height, aspect=6, hue='hue',sharex=False, sharey=False)\n",
    "    g.map(sns.kdeplot, \"value\", fill=True)\n",
    "    if region_names: g.fig.legend(region_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_egypt_shrub = region_train_3 & (data_index_2['Biome_obs']==16)\n",
    "region_egypt_desert = region_train_3 & (data_index_2['Biome_obs']==17)\n",
    "show_feature_distribution(features_nonweather, [region_egypt_shrub, region_egypt_desert], ['egyptian shrub','egyptian desert'])\n",
    "save_plot(plt.gcf(), 'egypt_shrub_desert_soil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_region(region_train_3 | region_test_3, plot_name='egypt_libya')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the action starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_3, region_test_3, objective_list=biome_list_3, feature_plots=True, experiment_name='s3_basic', plotmap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now run a series of tests, collecting data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run a series of tests, collecting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "performance = model_run(region_train_3, region_test_3, objective_list=biome_list_3, experiment_name='s3_basic')\n",
    "simulation_comparisons_3 = pd.DataFrame([['base']+performance], columns=['experiment name','accuracy, train', 'balanced accuracy, train', 'cross val accuracy, train',\n",
    "            'accuracy, test', 'balanced accuracy, test'])\n",
    "\n",
    "for i,feature_name in enumerate(features_names):\n",
    "    display(Markdown('---'))\n",
    "    print(f'Dropping season {feature_name}')\n",
    "    print(f'We dropped the features: {drop_features[i]}')\n",
    "    performance = model_run(region_train_3, region_test_3, objective_list=biome_list_3, \\\n",
    "               drop_columns=drop_features[i], experiment_name=f's3_drop_{feature_name.replace(\"|\",\"_\")}')\n",
    "    \n",
    "    simulation_comparisons_3 = pd.concat([simulation_comparisons_3, pd.DataFrame([[f'drop {feature_name}']+performance], columns=list(simulation_comparisons_3))])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_comparisons_3.to_pickle('../data/simulation_comparisons_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparisons_data(simulation_comparisons, experiment_name=None):\n",
    "    simulation_comparisons['error rate, test']= 1-simulation_comparisons['accuracy, test']\n",
    "    ax = sns.barplot(simulation_comparisons[['experiment name','error rate, test']], x='experiment name', y='error rate, test')\n",
    "    ax.set_ylabel('Error rate')\n",
    "    ax.set_xlabel('')\n",
    "    plt.axhline(y=simulation_comparisons['error rate, test'][0], color='black',linestyle='--')\n",
    "    plt.xticks(rotation=90);\n",
    "    if experiment_name: save_plot(plt.gcf(), f'{experiment_name}_simulationComparisons_errorRate')\n",
    "\n",
    "    plt.subplots()\n",
    "    simulation_comparisons['balanced error rate, test'] = 1-simulation_comparisons['balanced accuracy, test']\n",
    "    ax = sns.barplot(simulation_comparisons[['experiment name','balanced error rate, test']], x='experiment name', y='balanced error rate, test')\n",
    "    ax.set_ylabel('Balanced error rate')\n",
    "    ax.set_xlabel('')\n",
    "    plt.axhline(y=simulation_comparisons['balanced error rate, test'][0], color='black',linestyle='--')\n",
    "    plt.xticks(rotation=90);\n",
    "    if experiment_name: save_plot(plt.gcf(), f'{experiment_name}_simulationComparisons_balancedErrorRate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_comparisons_3 = simulation_comparisons_3.reset_index(drop=True)\n",
    "plot_comparisons_data(simulation_comparisons_3, 's3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the significant negative impact on the learning outcome by choosing regions at very different latitudes we chose as regions\n",
    "- for training: Russia\n",
    "- for testing: Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_train_4 = data_index_2['Pan_2007'] == 'Russia'\n",
    "region_test_4 = data_index_2['Pan_2007'] == 'Canada'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "data1 = data_index_2.loc[region_train_4] \n",
    "data2 = data_index_2.loc[region_test_4]\n",
    "\n",
    "data = pd.concat([data1,data2])\n",
    "plot_statistics(data, name_data = 'Section4Canada_Russia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_climate_diagramm(region_train_4,'russia')\n",
    "plot_climate_diagramm(region_test_4,'canada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_region(region_train_4 | region_test_4, plot_name='Russia_Canada')\n",
    "plot_region(region_train_4 | region_test_4, parameter='Biome_Cmax', plot_name='Russia_Canada_cmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, experiment_name='s4_basic', feature_plots=True, plotmap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is relatively bad performance. Fiddling with the hyperparameters changes a lot though the\n",
    "hyperparameter optimisation is far from optimal. Let's see how good it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, hyperparameter_tuning=True, experiment_name = 'Tuning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not too good in this case. But when we were previously comparing Africa with China fiddling with the parameters actually did improve things. In the following we will thus disable the hyperparameter tuning. Let's try dropping various features and see how that impacts performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "performance = model_run(region_train_4, region_test_4, experiment_name='s4_basic')\n",
    "simulation_comparisons_4 = pd.DataFrame([['base']+performance], columns=['experiment name','accuracy, train', 'balanced accuracy, train', 'cross val accuracy, train',\n",
    "            'accuracy, test', 'balanced accuracy, test'])\n",
    "\n",
    "for i,feature_name in enumerate(features_names):\n",
    "    display(Markdown('---'))\n",
    "    print(f'Dropping season {feature_name}')\n",
    "    print(f'We dropped the features: {drop_features[i]}')\n",
    "    performance = model_run(region_train_4, region_test_4, \\\n",
    "               drop_columns=drop_features[i], experiment_name=f's4_drop_{feature_name.replace(\"|\",\"_\")}')\n",
    "    \n",
    "    simulation_comparisons_4 = pd.concat([simulation_comparisons_4, pd.DataFrame([[f'drop {feature_name}']+performance], columns=list(simulation_comparisons_4))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_comparisons_4 = simulation_comparisons_4.reset_index(drop=True)\n",
    "simulation_comparisons_4.to_pickle('../data/simulation_comparisons_4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparisons_data(simulation_comparisons_4, 's4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how things are with biome_Cmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, feature_plots = True, plotmap=True, objective='Biome_Cmax', experiment_name = 'Basic_with_Biome_Cmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, hyperparameter_tuning=True, feature_plots = False, objective='Biome_Cmax', experiment_name = 's4Basic_with_Biome_Cmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "performance = model_run(region_train_4, region_test_4, objective='Biome_Cmax', experiment_name='s4_basic')\n",
    "simulation_comparisons_4 = pd.DataFrame([['base']+performance], columns=['experiment name','accuracy, train', 'balanced accuracy, train', 'cross val accuracy, train',\n",
    "            'accuracy, test', 'balanced accuracy, test'])\n",
    "\n",
    "for i,feature_name in enumerate(features_names):\n",
    "    display(Markdown('---'))\n",
    "    print(f'Dropping season {feature_name}')\n",
    "    print(f'We dropped the features: {drop_features[i]}')\n",
    "    performance = model_run(region_train_4, region_test_4, objective='Biome_Cmax',\\\n",
    "               drop_columns=drop_features[i], experiment_name=f's4_drop_{feature_name.replace(\"|\",\"_\")}')\n",
    "    \n",
    "    simulation_comparisons_4 = pd.concat([simulation_comparisons_4, pd.DataFrame([[f'drop {feature_name}']+performance], columns=list(simulation_comparisons_4))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_comparisons_4 = simulation_comparisons_4.reset_index(drop=True)\n",
    "simulation_comparisons_4.to_pickle('../data/simulation_comparisons_4_Cmax.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparisons_data(simulation_comparisons_4, 's4_cCmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for different features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, drop_columns=features_weather, objective='Biome_Cmax', experiment_name='s4_with_Biome_dropWeather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, use_columns=features_weather, objective='Biome_Cmax', experiment_name='s4_with_Biome_useWeather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is quite decent performance though not as good as with biome_obs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.6\n",
    "\n",
    "Test the model trained with 'Biome_Cmax' on 'Biome_obs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, objective='Biome_Cmax', objective_test='Biome_obs', plotmap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who would have thought? The model performs as badly as an antigen test measuring temperature. Now test the model trained on 'Biome_obs' on 'Biome_Cmax' (and expect a similar result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_4, region_test_4, objective='Biome_obs', objective_test='Biome_Cmax', \\\n",
    "          plotmap=True, experiment_name='s4_trainObs_testCmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who would have thought that our model works badly in this case (duh)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we creatively use the same test and validation set as in part 4. Though we swap order because of performance issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_train_5 = data_index_2['Pan_2007'] == 'Canada'\n",
    "region_test_5 = data_index_2['Pan_2007'] == 'Russia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "data1 = data_index_2.loc[region_train_5] \n",
    "data2 = data_index_2.loc[region_test_5]\n",
    "\n",
    "data = pd.concat([data1,data2])\n",
    "plot_statistics(data, name_data = 'Section5Canada_Russia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feature_distribution(['NPP','VegC'], [region_train_5, region_test_5], ['canada','russia'], height=2)\n",
    "fig=plt.gcf()\n",
    "fig.axes[0].set_xlim(xmin=0)\n",
    "fig.axes[1].set_xlim(xmin=0)\n",
    "save_plot(plt.gcf(), 'npp_vegc_distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_5, region_test_5, objective='NPP', continuous_Y=True, plotmap=True, feature_plots=True, experiment_name='s5_npp_basic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pretty decent outcome. Let's see how it behaves with 'VegC'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(region_train_5, region_test_5, objective='VegC', continuous_Y=True, feature_plots=True, experiment_name='s5_vegc_basic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'VegC' seems to perform worse. In the following we focus on NPP. What happens if we drop the medians?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "performance = model_run(region_train_5, region_test_5, experiment_name='s5_basic', continuous_Y=True)\n",
    "simulation_comparisons_5 = pd.DataFrame([['base']+performance], columns=['experiment name', 'R^2 score, train', 'score, test',\n",
    "             'MSE, test', 'MSE sktfct, test', 'sqrt(MSE)', 'max err', 'mean abs err'])\n",
    "\n",
    "for i,feature_name in enumerate(features_names):\n",
    "    display(Markdown('---'))\n",
    "    print(f'Dropping season {feature_name}')\n",
    "    print(f'We dropped the features: {drop_features[i]}')\n",
    "    performance = model_run(region_train_5, region_test_5, continuous_Y=True, \\\n",
    "               drop_columns=drop_features[i], experiment_name=f's5_npp_drop_{feature_name.replace(\"|\",\"_\")}')\n",
    "    \n",
    "    simulation_comparisons_5 = pd.concat([simulation_comparisons_5, pd.DataFrame([[f'drop {feature_name}']+performance], columns=list(simulation_comparisons_5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_comparisons_5 = simulation_comparisons_5.reset_index(drop=True)\n",
    "simulation_comparisons_5.to_pickle('../data/simulation_comparisons_5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(simulation_comparisons_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparisons_data_regr(simulation_comparisons, experiment_name=None):\n",
    "    simulation_comparisons['1-score, test']= 1-simulation_comparisons['score, test']\n",
    "    labels = ['1-score, test','MSE, test', 'MSE sktfct, test', 'max err', 'mean abs err']\n",
    "    ylabels = ['1-($R^2$ score)', 'MSE', 'MSE sktfct', 'Maximal error', 'Mean absolute error']\n",
    "    plt_names = ['1_R2score','MSE','MSE_sktfct','MaxErr','MeanAbsErr']\n",
    "    for i in range(len(labels)):\n",
    "        plt.subplots(figsize=(5,4))\n",
    "        ax = sns.barplot(simulation_comparisons[['experiment name',labels[i]]], x='experiment name', y=labels[i])\n",
    "        ax.set_ylabel(ylabels[i])\n",
    "        ax.set_xlabel('')\n",
    "        plt.axhline(y=simulation_comparisons[labels[i]].iloc[0], color='black',linestyle='--')\n",
    "        plt.xticks(rotation=90);\n",
    "        if experiment_name: save_plot(plt.gcf(), f'{experiment_name}_simulationComparisons_{plt_names[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparisons_data_regr(simulation_comparisons_5, 's5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_latex(simulation_comparisons_5.drop(columns=['R^2 score, train', 'MSE, test', 'MSE sktfct, test']), 's5_experiment_comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
